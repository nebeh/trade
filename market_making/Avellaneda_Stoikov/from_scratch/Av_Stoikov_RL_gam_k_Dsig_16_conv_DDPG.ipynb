{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time     open     high      low    close    volume  trades\n",
      "0 2025-03-18 20:03:00  1905.24  1905.49  1904.36  1904.65   90.7889   655.0\n",
      "1 2025-03-18 20:04:00  1904.64  1906.69  1904.64  1906.69  234.7526   926.0\n",
      "2 2025-03-18 20:05:00  1906.69  1907.56  1906.57  1906.64  147.4473   949.0\n",
      "3 2025-03-18 20:06:00  1906.65  1906.87  1905.36  1906.56   79.5244  1108.0\n",
      "4 2025-03-18 20:07:00  1906.56  1907.39  1905.85  1907.01  109.2926  1188.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from binance import Client\n",
    "from gym import spaces\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Получение данных (свечи) с Binance\n",
    "# =============================================================================\n",
    "client = Client()  # публичный клиент\n",
    "\n",
    "\n",
    "def get_klines(\n",
    "    symbol=\"ETHUSDT\",\n",
    "    interval=\"1m\",\n",
    "    start_date=\"6 day ago UTC\",\n",
    "    end_date=\"4 day ago UTC\",\n",
    "):\n",
    "    try:\n",
    "        klines = client.get_historical_klines(\n",
    "            symbol=symbol, interval=interval, start_str=start_date, end_str=end_date\n",
    "        )\n",
    "        columns = [\n",
    "            \"time\",\n",
    "            \"open\",\n",
    "            \"high\",\n",
    "            \"low\",\n",
    "            \"close\",\n",
    "            \"volume\",\n",
    "            \"close_time\",\n",
    "            \"quote_volume\",\n",
    "            \"trades\",\n",
    "            \"taker_buy_base\",\n",
    "            \"taker_buy_quote\",\n",
    "            \"ignore\",\n",
    "        ]\n",
    "        df = pd.DataFrame(klines, columns=columns, dtype=float)\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"ms\")\n",
    "        return df[[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при получении данных: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "df = get_klines(symbol=\"ETHUSDT\", interval=\"1m\")\n",
    "print(df.head())\n",
    "\n",
    "if df is not None:\n",
    "    N = len(df)\n",
    "    ratio = 0.8\n",
    "    df_test = df[int(N * ratio) :].reset_index(drop=True)\n",
    "    df = df[: int(N * ratio)].reset_index(drop=True)\n",
    "else:\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]\n",
    "    )\n",
    "    df_test = pd.DataFrame(\n",
    "        columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strike/work/penv/deep/lib/python3.13/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение DQN-агента...\n",
      "DQN Эпизод 1/20: суммарное вознаграждение = -245.53\n",
      "DQN Эпизод 2/20: суммарное вознаграждение = -1036.89\n",
      "DQN Эпизод 3/20: суммарное вознаграждение = 73.37\n",
      "DQN Эпизод 4/20: суммарное вознаграждение = 425.35\n",
      "DQN Эпизод 5/20: суммарное вознаграждение = -498.92\n",
      "DQN Эпизод 6/20: суммарное вознаграждение = 375.87\n",
      "DQN Эпизод 7/20: суммарное вознаграждение = -302.34\n",
      "DQN Эпизод 8/20: суммарное вознаграждение = -666.54\n",
      "DQN Эпизод 9/20: суммарное вознаграждение = 535.77\n",
      "DQN Эпизод 10/20: суммарное вознаграждение = 634.01\n",
      "DQN Эпизод 11/20: суммарное вознаграждение = -337.20\n",
      "DQN Эпизод 12/20: суммарное вознаграждение = 342.84\n",
      "DQN Эпизод 13/20: суммарное вознаграждение = -182.06\n",
      "DQN Эпизод 14/20: суммарное вознаграждение = -89.78\n",
      "DQN Эпизод 15/20: суммарное вознаграждение = 321.39\n",
      "DQN Эпизод 16/20: суммарное вознаграждение = 709.24\n",
      "DQN Эпизод 17/20: суммарное вознаграждение = -457.70\n",
      "DQN Эпизод 18/20: суммарное вознаграждение = 1110.99\n",
      "DQN Эпизод 19/20: суммарное вознаграждение = 581.02\n",
      "DQN Эпизод 20/20: суммарное вознаграждение = 720.00\n",
      "\n",
      "Обучение A2C-агента...\n",
      "A2C Эпизод 1/20: суммарное вознаграждение = 289.06\n",
      "A2C Эпизод 2/20: суммарное вознаграждение = -633.09\n",
      "A2C Эпизод 3/20: суммарное вознаграждение = -562.84\n",
      "A2C Эпизод 4/20: суммарное вознаграждение = 547.31\n",
      "A2C Эпизод 5/20: суммарное вознаграждение = 300.58\n",
      "A2C Эпизод 6/20: суммарное вознаграждение = -325.04\n",
      "A2C Эпизод 7/20: суммарное вознаграждение = 386.42\n",
      "A2C Эпизод 8/20: суммарное вознаграждение = 404.37\n",
      "A2C Эпизод 9/20: суммарное вознаграждение = 925.96\n",
      "A2C Эпизод 10/20: суммарное вознаграждение = 1339.81\n",
      "A2C Эпизод 11/20: суммарное вознаграждение = 1354.69\n",
      "A2C Эпизод 12/20: суммарное вознаграждение = 456.56\n",
      "A2C Эпизод 13/20: суммарное вознаграждение = -302.68\n",
      "A2C Эпизод 14/20: суммарное вознаграждение = -184.60\n",
      "A2C Эпизод 15/20: суммарное вознаграждение = -243.21\n",
      "A2C Эпизод 16/20: суммарное вознаграждение = 85.60\n",
      "A2C Эпизод 17/20: суммарное вознаграждение = 889.64\n",
      "A2C Эпизод 18/20: суммарное вознаграждение = 135.64\n",
      "A2C Эпизод 19/20: суммарное вознаграждение = 750.24\n",
      "A2C Эпизод 20/20: суммарное вознаграждение = -709.79\n",
      "\n",
      "Обучение PPO-агента...\n",
      "PPO Эпизод 1/20: суммарное вознаграждение = 324.95\n",
      "PPO Эпизод 2/20: суммарное вознаграждение = -653.47\n",
      "PPO Эпизод 3/20: суммарное вознаграждение = -127.13\n",
      "PPO Эпизод 4/20: суммарное вознаграждение = 73.46\n",
      "PPO Эпизод 5/20: суммарное вознаграждение = 138.88\n",
      "PPO Эпизод 6/20: суммарное вознаграждение = 58.60\n",
      "PPO Эпизод 7/20: суммарное вознаграждение = 89.37\n",
      "PPO Эпизод 8/20: суммарное вознаграждение = 352.92\n",
      "PPO Эпизод 9/20: суммарное вознаграждение = -1494.93\n",
      "PPO Эпизод 10/20: суммарное вознаграждение = -153.27\n",
      "PPO Эпизод 11/20: суммарное вознаграждение = 763.52\n",
      "PPO Эпизод 12/20: суммарное вознаграждение = -197.02\n",
      "PPO Эпизод 13/20: суммарное вознаграждение = -626.11\n",
      "PPO Эпизод 14/20: суммарное вознаграждение = 630.54\n",
      "PPO Эпизод 15/20: суммарное вознаграждение = 388.61\n",
      "PPO Эпизод 16/20: суммарное вознаграждение = -480.96\n",
      "PPO Эпизод 17/20: суммарное вознаграждение = -104.51\n",
      "PPO Эпизод 18/20: суммарное вознаграждение = 31.58\n",
      "PPO Эпизод 19/20: суммарное вознаграждение = -273.63\n",
      "PPO Эпизод 20/20: суммарное вознаграждение = 413.35\n",
      "\n",
      "Обучение DDPG-агента...\n",
      "DDPG Эпизод 1/20: суммарное вознаграждение = -29.77\n",
      "DDPG Эпизод 2/20: суммарное вознаграждение = 5.05\n",
      "DDPG Эпизод 3/20: суммарное вознаграждение = 34.76\n",
      "DDPG Эпизод 4/20: суммарное вознаграждение = -54.55\n",
      "DDPG Эпизод 5/20: суммарное вознаграждение = 2.43\n",
      "DDPG Эпизод 6/20: суммарное вознаграждение = 22.92\n",
      "DDPG Эпизод 7/20: суммарное вознаграждение = -19.23\n",
      "DDPG Эпизод 8/20: суммарное вознаграждение = 79.67\n",
      "DDPG Эпизод 9/20: суммарное вознаграждение = 0.00\n",
      "DDPG Эпизод 10/20: суммарное вознаграждение = -27.77\n",
      "DDPG Эпизод 11/20: суммарное вознаграждение = 27.07\n",
      "DDPG Эпизод 12/20: суммарное вознаграждение = 50.01\n",
      "DDPG Эпизод 13/20: суммарное вознаграждение = 63.40\n",
      "DDPG Эпизод 14/20: суммарное вознаграждение = 12.79\n",
      "DDPG Эпизод 15/20: суммарное вознаграждение = -165.11\n",
      "DDPG Эпизод 16/20: суммарное вознаграждение = 69.67\n",
      "DDPG Эпизод 17/20: суммарное вознаграждение = -21.93\n",
      "DDPG Эпизод 18/20: суммарное вознаграждение = 53.55\n",
      "DDPG Эпизод 19/20: суммарное вознаграждение = 114.31\n",
      "DDPG Эпизод 20/20: суммарное вознаграждение = -139.15\n",
      "\n",
      "Результаты тестирования:\n",
      "DQN итоговый PnL: 0.00, Max Drawdown: 0.00, Sharpe Ratio: 0.00\n",
      "A2C итоговый PnL: 53.92, Max Drawdown: -38.36, Sharpe Ratio: 0.72\n",
      "PPO итоговый PnL: -149.95, Max Drawdown: -441.16, Sharpe Ratio: -0.46\n",
      "DDPG итоговый PnL: 0.00, Max Drawdown: 0.00, Sharpe Ratio: 0.00\n"
     ]
    }
   ],
   "source": [
    "gamma_range = [0.005, 1.0]\n",
    "kappa_range = [1e-4, 1e-2]\n",
    "vol_corr_range = [-0.01, 0.01]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Определение среды MarketMakingEnv\n",
    "# =============================================================================\n",
    "class MarketMakingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Окружение маркет-мейкинга по Avellaneda-Stoikov с использованием свечных данных.\n",
    "    Состояние: [inventory, price history (относительные изменения цены, длина=history_len), PnL].\n",
    "    Действие:\n",
    "      - Если continuous=False: мультидискретный выбор (γ, κ, Δσ) из заданных списков.\n",
    "      - Если continuous=True: непрерывное действие – вектор [γ, κ, Δσ] из заданных диапазонов.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        gamma_values=None,\n",
    "        kappa_values=None,\n",
    "        vol_corr_values=None,\n",
    "        T=60.0,\n",
    "        history_len=16,\n",
    "        continuous=False,\n",
    "    ):\n",
    "        super(MarketMakingEnv, self).__init__()\n",
    "        self.prices = df.close.values\n",
    "        self.prices_high = df.high.values\n",
    "        self.prices_low = df.low.values\n",
    "        self.N = len(df)\n",
    "        self.T = T  # торговый горизонт (сек)\n",
    "        self.history_len = history_len\n",
    "        self.continuous = continuous\n",
    "        self.initial_cash = 10000.0\n",
    "        self.fee_rate = 0.0\n",
    "        if continuous:\n",
    "            self.action_space = spaces.Box(\n",
    "                low=np.array(\n",
    "                    [gamma_range[0], kappa_range[0], vol_corr_range[0]],\n",
    "                    dtype=np.float32,\n",
    "                ),\n",
    "                high=np.array(\n",
    "                    [gamma_range[1], kappa_range[1], vol_corr_range[1]],\n",
    "                    dtype=np.float32,\n",
    "                ),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "        else:\n",
    "            self.gamma_values = gamma_values\n",
    "            self.kappa_values = kappa_values\n",
    "            self.vol_corr_values = vol_corr_values\n",
    "            self.action_space = spaces.MultiDiscrete(\n",
    "                [len(gamma_values), len(kappa_values), len(vol_corr_values)]\n",
    "            )\n",
    "        # Размер наблюдения: [inventory (1), price history (history_len), PnL (1)] = 1 + history_len + 1\n",
    "        low = np.concatenate(\n",
    "            (np.array([-100.0]), np.full((history_len,), -1.0), np.array([-1e9]))\n",
    "        )\n",
    "        high = np.concatenate(\n",
    "            (np.array([100.0]), np.full((history_len,), 1.0), np.array([1e9]))\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.inventory = 0.0\n",
    "        self.cash = self.initial_cash\n",
    "        self.price_change_history = [0.0] * self.history_len\n",
    "        pnl = self.cash - self.initial_cash\n",
    "        state = np.concatenate(\n",
    "            ([self.inventory], np.array(self.price_change_history), [pnl])\n",
    "        )\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Определяем параметры из действия\n",
    "        if self.continuous:\n",
    "            gamma = float(action[0])\n",
    "            kappa = float(action[1])\n",
    "            vol_corr = float(action[2])\n",
    "        else:\n",
    "            n_gamma = len(self.gamma_values)\n",
    "            n_kappa = len(self.kappa_values)\n",
    "            n_vol = len(self.vol_corr_values)\n",
    "            if isinstance(action, (np.ndarray, list, tuple)):\n",
    "                gamma_idx = int(action[0])\n",
    "                kappa_idx = int(action[1])\n",
    "                vol_corr_idx = int(action[2])\n",
    "            else:\n",
    "                action = int(action)\n",
    "                gamma_idx = action // (n_kappa * n_vol)\n",
    "                rem = action % (n_kappa * n_vol)\n",
    "                kappa_idx = rem // n_vol\n",
    "                vol_corr_idx = rem % n_vol\n",
    "            gamma = self.gamma_values[gamma_idx]\n",
    "            kappa = self.kappa_values[kappa_idx]\n",
    "            vol_corr = self.vol_corr_values[vol_corr_idx]\n",
    "        mid_price = self.prices[self.t]\n",
    "        sigma_est = 0.0\n",
    "        if self.t > 1:\n",
    "            start = max(0, self.t - int(self.T))\n",
    "            window_prices = self.prices[start : self.t + 1]\n",
    "            if len(window_prices) > 1:\n",
    "                returns = np.diff(window_prices) / window_prices[:-1]\n",
    "                sigma_est = np.std(returns)\n",
    "        effective_sigma = sigma_est + vol_corr\n",
    "        if effective_sigma < 1e-8:\n",
    "            effective_sigma = 1e-8\n",
    "        reservation_price = (\n",
    "            mid_price - self.inventory * gamma * (effective_sigma**2) * self.T\n",
    "        )\n",
    "        delta = gamma * (effective_sigma**2) * self.T + (1.0 / gamma) * math.log(\n",
    "            1 + gamma / kappa\n",
    "        )\n",
    "        bid_price = reservation_price - delta / 2\n",
    "        ask_price = reservation_price + delta / 2\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "        if self.t < self.N - 1:\n",
    "            next_price = self.prices[self.t + 1]\n",
    "            if self.prices_high[self.t + 1] >= ask_price:\n",
    "                self.inventory -= 1.0\n",
    "                self.cash += ask_price * (1.0 - self.fee_rate)\n",
    "            if self.prices_low[self.t + 1] <= bid_price:\n",
    "                self.inventory += 1.0\n",
    "                self.cash -= bid_price * (1.0 + self.fee_rate)\n",
    "            current_value = self.cash + self.inventory * next_price\n",
    "            prev_value = self.cash + self.inventory * mid_price\n",
    "            pnl_change = current_value - prev_value\n",
    "            reward = pnl_change - 0.001 * abs(self.inventory)\n",
    "            self.t += 1\n",
    "        else:\n",
    "            done = True\n",
    "            current_value = self.cash + self.inventory * mid_price\n",
    "            reward = current_value - self.initial_cash\n",
    "        if self.t > 0:\n",
    "            rel_change = (self.prices[self.t] - self.prices[self.t - 1]) / self.prices[\n",
    "                self.t - 1\n",
    "            ]\n",
    "        else:\n",
    "            rel_change = 0.0\n",
    "        self.price_change_history.pop(0)\n",
    "        self.price_change_history.append(rel_change)\n",
    "        if not done:\n",
    "            new_mid = self.prices[self.t]\n",
    "            pnl = self.cash + self.inventory * new_mid - self.initial_cash\n",
    "            obs = np.concatenate(\n",
    "                ([self.inventory], np.array(self.price_change_history), [pnl])\n",
    "            )\n",
    "            obs = obs.astype(np.float32)\n",
    "        else:\n",
    "            obs = None\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Нейросетевая архитектура: CombinedNetwork\n",
    "# =============================================================================\n",
    "class CombinedNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        history_len=16,\n",
    "        inv_pnl_dim=2,\n",
    "        conv_out_channels=32,\n",
    "        fc_inv_out=32,\n",
    "        fc1_out=64,\n",
    "        fc2_out=64,\n",
    "    ):\n",
    "        super(CombinedNetwork, self).__init__()\n",
    "        # Сверточная ветвь для price history (вход: [batch, 1, history_len])\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=16, out_channels=conv_out_channels, kernel_size=3\n",
    "        )\n",
    "        # Выходной размер сверточной части: conv_out_channels * (history_len - 4)\n",
    "        conv_feat_dim = conv_out_channels * (history_len - 4)\n",
    "        # Полносвязная ветвь для inventory и PnL (2 значения)\n",
    "        self.fc_inv = nn.Linear(inv_pnl_dim, fc_inv_out)\n",
    "        # Основная часть объединяет выходы сверточной и MLP ветвей\n",
    "        combined_dim = conv_feat_dim + fc_inv_out\n",
    "        self.fc1 = nn.Linear(combined_dim, fc1_out)\n",
    "        self.fc2 = nn.Linear(fc1_out, fc2_out)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # state: [batch, 18] = [inventory (1), price_history (16), pnl (1)]\n",
    "        inv = state[:, 0:1]  # [batch, 1]\n",
    "        price_history = state[:, 1:17]  # [batch, 16]\n",
    "        pnl = state[:, 17:18]  # [batch, 1]\n",
    "        inv_pnl = torch.cat([inv, pnl], dim=1)  # [batch, 2]\n",
    "        x_price = price_history.unsqueeze(1)  # [batch, 1, 16]\n",
    "        x_price = torch.relu(self.conv1(x_price))  # -> [batch, 16, 14]\n",
    "        x_price = torch.relu(self.conv2(x_price))  # -> [batch, conv_out_channels, 12]\n",
    "        x_price = x_price.view(x_price.size(0), -1)  # [batch, conv_out_channels*12]\n",
    "        x_inv = torch.relu(self.fc_inv(inv_pnl))  # [batch, fc_inv_out]\n",
    "        x = torch.cat([x_price, x_inv], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x  # [batch, fc2_out]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. CombinedQNetwork для DQN\n",
    "# =============================================================================\n",
    "class CombinedQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, history_len=16):\n",
    "        super(CombinedQNetwork, self).__init__()\n",
    "        self.base = CombinedNetwork(\n",
    "            history_len=history_len,\n",
    "            inv_pnl_dim=2,\n",
    "            conv_out_channels=32,\n",
    "            fc_inv_out=32,\n",
    "            fc1_out=64,\n",
    "            fc2_out=64,\n",
    "        )\n",
    "        self.out = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.base(state)\n",
    "        q_values = self.out(x)\n",
    "        return q_values\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. CombinedActorCritic для A2C и PPO\n",
    "# =============================================================================\n",
    "class CombinedActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, history_len=16):\n",
    "        super(CombinedActorCritic, self).__init__()\n",
    "        self.base = CombinedNetwork(\n",
    "            history_len=history_len,\n",
    "            inv_pnl_dim=2,\n",
    "            conv_out_channels=32,\n",
    "            fc_inv_out=32,\n",
    "            fc1_out=64,\n",
    "            fc2_out=64,\n",
    "        )\n",
    "        self.actor = nn.Linear(64, action_dim)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.base(state)\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7. DQNAgent (дискретный режим)\n",
    "# =============================================================================\n",
    "Transition = namedtuple(\n",
    "    \"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        buffer_capacity=10000,\n",
    "        batch_size=64,\n",
    "        target_update=1000,\n",
    "        history_len=16,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = CombinedQNetwork(\n",
    "            state_dim, action_dim, history_len=history_len\n",
    "        ).to(self.device)\n",
    "        self.target_network = CombinedQNetwork(\n",
    "            state_dim, action_dim, history_len=history_len\n",
    "        ).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.steps_done = 0\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def push_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.FloatTensor(batch.state).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).unsqueeze(1).to(self.device)\n",
    "        non_final_mask = torch.tensor(\n",
    "            [s is not None for s in batch.next_state],\n",
    "            dtype=torch.bool,\n",
    "            device=self.device,\n",
    "        )\n",
    "        non_final_next_states = torch.FloatTensor(\n",
    "            [s for s in batch.next_state if s is not None]\n",
    "        ).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch.done).unsqueeze(1).to(self.device)\n",
    "        q_values = self.q_network(state_batch).gather(1, action_batch)\n",
    "        next_q_values = torch.zeros(self.batch_size, 1).to(self.device)\n",
    "        if non_final_next_states.size(0) > 0:\n",
    "            next_q_values[non_final_mask] = self.target_network(\n",
    "                non_final_next_states\n",
    "            ).max(1, keepdim=True)[0]\n",
    "        expected_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
    "        loss = nn.MSELoss()(q_values, expected_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.steps_done += 1\n",
    "        if self.steps_done % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8. A2CAgent (дискретный режим)\n",
    "# =============================================================================\n",
    "class A2CAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "        history_len=16,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = CombinedActorCritic(\n",
    "            state_dim, action_dim, history_len=history_len\n",
    "        ).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, value = self.model(state_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        action_tensor = torch.multinomial(probs, num_samples=1)\n",
    "        log_prob = torch.log(probs.gather(1, action_tensor) + 1e-10)\n",
    "        return (\n",
    "            action_tensor.item(),\n",
    "            log_prob.squeeze(),\n",
    "            value.squeeze(),\n",
    "            logits.squeeze(),\n",
    "        )\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        log_probs = torch.stack([t[2] for t in trajectories]).to(self.device)\n",
    "        values = torch.stack([t[3] for t in trajectories]).to(self.device)\n",
    "        rewards = [t[4] for t in trajectories]\n",
    "        dones = [t[5] for t in trajectories]\n",
    "        logits_list = torch.stack([t[6] for t in trajectories]).to(self.device)\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = returns - values\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        probs = torch.softmax(logits_list, dim=1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 9. PPOAgent (дискретный режим)\n",
    "# =============================================================================\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        clip_coef=0.2,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "        ppo_epochs=4,\n",
    "        history_len=16,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = CombinedActorCritic(\n",
    "            state_dim, action_dim, history_len=history_len\n",
    "        ).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_coef = clip_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, value = self.model(state_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        action_tensor = torch.multinomial(probs, num_samples=1)\n",
    "        log_prob = torch.log(probs.gather(1, action_tensor) + 1e-10)\n",
    "        return action_tensor.item(), log_prob.squeeze(), value.squeeze()\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        states = torch.FloatTensor([t[0] for t in trajectories]).to(self.device)\n",
    "        actions = (\n",
    "            torch.LongTensor([t[1] for t in trajectories]).unsqueeze(1).to(self.device)\n",
    "        )\n",
    "        old_log_probs = torch.stack([t[2] for t in trajectories]).to(self.device)\n",
    "        old_values = torch.stack([t[3] for t in trajectories]).to(self.device)\n",
    "        rewards = [t[4] for t in trajectories]\n",
    "        dones = [t[5] for t in trajectories]\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        old_log_probs = old_log_probs.detach()\n",
    "        old_values = old_values.detach()\n",
    "        advantages = returns - old_values\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            logits, values = self.model(states)\n",
    "            values = values.squeeze(1)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            new_log_probs = torch.log(probs.gather(1, actions) + 1e-10).squeeze(1)\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            adv_detached = advantages.detach()\n",
    "            surr1 = ratio * adv_detached\n",
    "            surr2 = (\n",
    "                torch.clamp(ratio, 1.0 - self.clip_coef, 1.0 + self.clip_coef)\n",
    "                * adv_detached\n",
    "            )\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (returns - values).pow(2).mean()\n",
    "            entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "            loss = (\n",
    "                actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "            )\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 10. DDPGAgent (непрерывный режим)\n",
    "# =============================================================================\n",
    "class CombinedActorDDPG(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim=3, history_len=16):\n",
    "        super(CombinedActorDDPG, self).__init__()\n",
    "        self.base = CombinedNetwork(\n",
    "            history_len=history_len,\n",
    "            inv_pnl_dim=2,\n",
    "            conv_out_channels=32,\n",
    "            fc_inv_out=32,\n",
    "            fc1_out=64,\n",
    "            fc2_out=64,\n",
    "        )\n",
    "        self.out = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.base(state)\n",
    "        a = torch.tanh(self.out(x))\n",
    "        # Масштабирование выхода в диапазоны: γ, κ, Δσ\n",
    "        gamma = gamma_range[0] + (gamma_range[1] - gamma_range[0]) * ((a[:, 0] + 1) / 2)\n",
    "        kappa = kappa_range[0] + (kappa_range[1] - kappa_range[0]) * ((a[:, 1] + 1) / 2)\n",
    "        vol_corr = vol_corr_range[0] + (vol_corr_range[1] - vol_corr_range[0]) * (\n",
    "            (a[:, 2] + 1) / 2\n",
    "        )\n",
    "        action_out = torch.stack([gamma, kappa, vol_corr], dim=1)\n",
    "        return action_out\n",
    "\n",
    "\n",
    "class CombinedCriticDDPG(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim=3, history_len=16):\n",
    "        super(CombinedCriticDDPG, self).__init__()\n",
    "        self.base = CombinedNetwork(\n",
    "            history_len=history_len,\n",
    "            inv_pnl_dim=2,\n",
    "            conv_out_channels=32,\n",
    "            fc_inv_out=32,\n",
    "            fc1_out=64,\n",
    "            fc2_out=64,\n",
    "        )\n",
    "        self.out = nn.Linear(64 + action_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = self.base(state)\n",
    "        x = torch.cat([x, action], dim=1)\n",
    "        q = self.out(x)\n",
    "        return q\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim=3,\n",
    "        lr_actor=1e-3,\n",
    "        lr_critic=1e-3,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        buffer_capacity=100000,\n",
    "        batch_size=64,\n",
    "        history_len=16,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.actor = CombinedActorDDPG(state_dim, action_dim, history_len).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.critic = CombinedCriticDDPG(state_dim, action_dim, history_len).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.target_actor = CombinedActorDDPG(state_dim, action_dim, history_len).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.target_critic = CombinedCriticDDPG(state_dim, action_dim, history_len).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau  # коэффициент для soft update\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=buffer_capacity)\n",
    "        self.Experience = namedtuple(\n",
    "            \"Experience\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "        )\n",
    "\n",
    "    def select_action(self, state, noise_scale=0.1):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state_tensor).cpu().data.numpy().flatten()\n",
    "        self.actor.train()\n",
    "        action += noise_scale * np.random.randn(len(action))\n",
    "        # Обеспечиваем, чтобы действие оставалось в заданном диапазоне\n",
    "        action[0] = np.clip(action[0], gamma_range[0], gamma_range[1])\n",
    "        action[1] = np.clip(action[1], kappa_range[0], kappa_range[1])\n",
    "        action[2] = np.clip(action[2], vol_corr_range[0], vol_corr_range[1])\n",
    "        return action\n",
    "\n",
    "    def push_experience(self, state, action, reward, next_state, done):\n",
    "        e = self.Experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(e)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        states = torch.FloatTensor([e.state for e in batch]).to(self.device)\n",
    "        actions = torch.FloatTensor([e.action for e in batch]).to(self.device)\n",
    "        rewards = (\n",
    "            torch.FloatTensor([e.reward for e in batch]).unsqueeze(1).to(self.device)\n",
    "        )\n",
    "        # Если next_state равен None, заменяем его нулевым вектором той же размерности, что state\n",
    "        next_states = torch.FloatTensor(\n",
    "            [\n",
    "                e.next_state if e.next_state is not None else np.zeros_like(e.state)\n",
    "                for e in batch\n",
    "            ]\n",
    "        ).to(self.device)\n",
    "        dones = torch.FloatTensor([e.done for e in batch]).unsqueeze(1).to(self.device)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        states, actions, rewards, next_states, dones = self.sample_batch()\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.target_actor(next_states)\n",
    "            target_q = self.target_critic(next_states, next_actions)\n",
    "            y = rewards + self.gamma * (1 - dones) * target_q\n",
    "        current_q = self.critic(states, actions)\n",
    "        critic_loss = nn.MSELoss()(current_q, y)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        pred_actions = self.actor(states)\n",
    "        actor_loss = -self.critic(states, pred_actions).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.soft_update(self.actor, self.target_actor)\n",
    "        self.soft_update(self.critic, self.target_critic)\n",
    "\n",
    "    def soft_update(self, source, target):\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                self.tau * source_param.data + (1 - self.tau) * target_param.data\n",
    "            )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 11. Метрики: максимальная просадка и коэффициент Шарпа\n",
    "# =============================================================================\n",
    "def compute_max_drawdown(pnl_history):\n",
    "    pnl_array = np.array(pnl_history)\n",
    "    running_max = np.maximum.accumulate(pnl_array)\n",
    "    drawdowns = pnl_array - running_max\n",
    "    return drawdowns.min()\n",
    "\n",
    "\n",
    "def compute_sharpe_ratio(pnl_history):\n",
    "    pnl_array = np.array(pnl_history)\n",
    "    returns = np.diff(pnl_array)\n",
    "    if returns.std() == 0:\n",
    "        return 0.0\n",
    "    sharpe = returns.mean() / returns.std() * np.sqrt(len(returns))\n",
    "    return sharpe\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 12. Параметры для дискретных агентов (DQN, A2C, PPO) и для DDPG\n",
    "# =============================================================================\n",
    "# Для дискретного режима:\n",
    "gamma_values = [0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "kappa_values = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
    "vol_corr_values = [-0.01, -0.005, 0.0, 0.005, 0.01]\n",
    "# Создаем среду для дискретных агентов:\n",
    "env = MarketMakingEnv(\n",
    "    df=df,\n",
    "    gamma_values=gamma_values,\n",
    "    kappa_values=kappa_values,\n",
    "    vol_corr_values=vol_corr_values,\n",
    "    T=60.0,\n",
    "    history_len=16,\n",
    "    continuous=False,\n",
    ")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "if isinstance(env.action_space, spaces.MultiDiscrete):\n",
    "    disc_action_dim = int(np.prod(env.action_space.nvec))\n",
    "else:\n",
    "    disc_action_dim = env.action_space.n\n",
    "# Для DDPG – непрерывный режим:\n",
    "env_cont = MarketMakingEnv(df=df, T=60.0, history_len=16, continuous=True)\n",
    "cont_action_dim = 3\n",
    "\n",
    "# =============================================================================\n",
    "# 13. Инициализация агентов\n",
    "# =============================================================================\n",
    "dqn_agent = DQNAgent(\n",
    "    state_dim,\n",
    "    disc_action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    buffer_capacity=10000,\n",
    "    batch_size=64,\n",
    "    target_update=1000,\n",
    "    history_len=16,\n",
    ")\n",
    "a2c_agent = A2CAgent(\n",
    "    state_dim,\n",
    "    disc_action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    value_coef=0.5,\n",
    "    entropy_coef=0.01,\n",
    "    history_len=16,\n",
    ")\n",
    "ppo_agent = PPOAgent(\n",
    "    state_dim,\n",
    "    disc_action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    clip_coef=0.2,\n",
    "    value_coef=0.5,\n",
    "    entropy_coef=0.01,\n",
    "    ppo_epochs=4,\n",
    "    history_len=16,\n",
    ")\n",
    "ddpg_agent = DDPGAgent(\n",
    "    state_dim,\n",
    "    action_dim=cont_action_dim,\n",
    "    lr_actor=1e-3,\n",
    "    lr_critic=1e-3,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    buffer_capacity=100000,\n",
    "    batch_size=64,\n",
    "    history_len=16,\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 14. Параметры обучения\n",
    "# =============================================================================\n",
    "num_episodes = 20  # для DQN\n",
    "num_episodes_a2c = 20  # для A2C\n",
    "num_episodes_ppo = 20  # для PPO\n",
    "num_episodes_ddpg = 20  # для DDPG\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 300\n",
    "\n",
    "# =============================================================================\n",
    "# 15. Обучение дискретных агентов (DQN, A2C, PPO)\n",
    "# =============================================================================\n",
    "print(\"Обучение DQN-агента...\")\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        epsilon = epsilon_final + (epsilon_start - epsilon_final) * math.exp(\n",
    "            -env.t / epsilon_decay\n",
    "        )\n",
    "        action = dqn_agent.select_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        dqn_agent.push_transition(state, action, reward, next_state, done)\n",
    "        dqn_agent.update()\n",
    "        state = next_state if next_state is not None else state\n",
    "        total_reward += reward\n",
    "    print(\n",
    "        f\"DQN Эпизод {episode+1}/{num_episodes}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nОбучение A2C-агента...\")\n",
    "rollout_length = 10\n",
    "for episode in range(num_episodes_a2c):\n",
    "    state = env.reset()\n",
    "    trajectories = []\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        for _ in range(rollout_length):\n",
    "            action, log_prob, value, logits = a2c_agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectories.append(\n",
    "                (state, action, log_prob, value, reward, float(done), logits)\n",
    "            )\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        a2c_agent.update(trajectories)\n",
    "        trajectories = []\n",
    "    print(\n",
    "        f\"A2C Эпизод {episode+1}/{num_episodes_a2c}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nОбучение PPO-агента...\")\n",
    "rollout_length_ppo = 20\n",
    "for episode in range(num_episodes_ppo):\n",
    "    state = env.reset()\n",
    "    trajectories = []\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        for _ in range(rollout_length_ppo):\n",
    "            action, log_prob, value = ppo_agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectories.append((state, action, log_prob, value, reward, float(done)))\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        ppo_agent.update(trajectories)\n",
    "        trajectories = []\n",
    "    print(\n",
    "        f\"PPO Эпизод {episode+1}/{num_episodes_ppo}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "# =============================================================================\n",
    "# 16. Обучение DDPG-агента (непрерывный режим)\n",
    "# =============================================================================\n",
    "print(\"\\nОбучение DDPG-агента...\")\n",
    "for episode in range(num_episodes_ddpg):\n",
    "    state = env_cont.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = ddpg_agent.select_action(state, noise_scale=0.1)\n",
    "        next_state, reward, done, _ = env_cont.step(action)\n",
    "        ddpg_agent.push_experience(state, action, reward, next_state, done)\n",
    "        ddpg_agent.update()\n",
    "        state = next_state if next_state is not None else state\n",
    "        total_reward += reward\n",
    "    print(\n",
    "        f\"DDPG Эпизод {episode+1}/{num_episodes_ddpg}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 17. Тестирование агентов и вычисление метрик\n",
    "# =============================================================================\n",
    "def run_agent(env, agent, agent_type=\"dqn\"):\n",
    "    state = env.reset()\n",
    "    pnl_history = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        if agent_type == \"dqn\":\n",
    "            action = agent.select_action(state, epsilon=0.0)\n",
    "        elif agent_type == \"a2c\":\n",
    "            action, _, _, _ = agent.select_action(state)\n",
    "        elif agent_type == \"ppo\":\n",
    "            action, _, _ = agent.select_action(state)\n",
    "        elif agent_type == \"ddpg\":\n",
    "            action = ddpg_agent.select_action(state, noise_scale=0.0)\n",
    "        else:\n",
    "            action = 0\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        if state is not None:\n",
    "            pnl_history.append(state[-1])\n",
    "        state = next_state if next_state is not None else state\n",
    "    return pnl_history\n",
    "\n",
    "\n",
    "env_test_disc = MarketMakingEnv(\n",
    "    df=df_test,\n",
    "    gamma_values=gamma_values,\n",
    "    kappa_values=kappa_values,\n",
    "    vol_corr_values=vol_corr_values,\n",
    "    T=60.0,\n",
    "    history_len=16,\n",
    "    continuous=False,\n",
    ")\n",
    "dqn_pnl = run_agent(env_test_disc, dqn_agent, agent_type=\"dqn\")\n",
    "a2c_pnl = run_agent(env_test_disc, a2c_agent, agent_type=\"a2c\")\n",
    "ppo_pnl = run_agent(env_test_disc, ppo_agent, agent_type=\"ppo\")\n",
    "env_test_cont = MarketMakingEnv(df=df_test, T=60.0, history_len=16, continuous=True)\n",
    "ddpg_pnl = run_agent(env_test_cont, ddpg_agent, agent_type=\"ddpg\")\n",
    "\n",
    "dqn_max_dd = compute_max_drawdown(dqn_pnl)\n",
    "dqn_sharpe = compute_sharpe_ratio(dqn_pnl)\n",
    "a2c_max_dd = compute_max_drawdown(a2c_pnl)\n",
    "a2c_sharpe = compute_sharpe_ratio(a2c_pnl)\n",
    "ppo_max_dd = compute_max_drawdown(ppo_pnl)\n",
    "ppo_sharpe = compute_sharpe_ratio(ppo_pnl)\n",
    "ddpg_max_dd = compute_max_drawdown(ddpg_pnl)\n",
    "ddpg_sharpe = compute_sharpe_ratio(ddpg_pnl)\n",
    "\n",
    "print(\"\\nРезультаты тестирования:\")\n",
    "print(\n",
    "    f\"DQN итоговый PnL: {dqn_pnl[-1]:.2f}, Max Drawdown: {dqn_max_dd:.2f}, Sharpe Ratio: {dqn_sharpe:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"A2C итоговый PnL: {a2c_pnl[-1]:.2f}, Max Drawdown: {a2c_max_dd:.2f}, Sharpe Ratio: {a2c_sharpe:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"PPO итоговый PnL: {ppo_pnl[-1]:.2f}, Max Drawdown: {ppo_max_dd:.2f}, Sharpe Ratio: {ppo_sharpe:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"DDPG итоговый PnL: {ddpg_pnl[-1]:.2f}, Max Drawdown: {ddpg_max_dd:.2f}, Sharpe Ratio: {ddpg_sharpe:.2f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
