{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Текущий подход по данным с дискретностью 1 мин\n",
    "(Минутные данные взяты для тестирования и проработки подхода) \n",
    "\n",
    "\n",
    "По модели Avellaneda-Stoikov делается расчет \n",
    "- реализованная цена\n",
    "$$r = S_t -q\\gamma\\sigma^2(T-t)$$\n",
    "- спред\n",
    "$$\\delta_a + \\delta_b = \\gamma\\sigma^2(T-t) + \\frac{1}{\\gamma}\\ln(1+\\frac{\\gamma}{k})$$\n",
    "\n",
    "### Параметр оптимизации (ищем RL методами), реализованный в текущем подходе\n",
    "- $\\gamma$ - уровень аверсии (ниприятия) к риску\n",
    "\n",
    "##### Какие еще параметры можем оптимизировать\n",
    "- T - время горизонта\n",
    "- k - параметр интенсивности сделок, который находится из соотношения $\\lambda(\\delta) = A\\exp(-k\\delta)$ (это если не находим этот параметр из реальных данных)\n",
    "- $\\Delta k$ поправку в параметр. Здесь мы принимаем, что $k = \\hat{k} + \\Delta k$, где $\\hat{k}$ оценка параметра из данных\n",
    "- $\\Delta\\sigma$ поправку в волатильность. Здесь мы принимаем, что $\\sigma = \\hat\\sigma + \\Delta\\sigma$, где $\\hat\\sigma$ оценка волатильности из данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дальнейшие улучшения \n",
    "1. Реализация на более высокочастотных таймфреймах\n",
    "2. Использовать книгу заказов в качестве observation для агента (скорее всего это сильно улучшит предсказательную способность модели)\n",
    "3. Реализовать RL схему, которая учит на прямую определять реализованную цену и спред (скорее всего результаты будут не устойчивые, но проверить можно, если давать агенту книгу заказов)\n",
    "4. Использовать более реалистичные модели рынка, например модель Мертона со скачками\n",
    "$$ dS_t = \\mu dt +\\sigma dW +JdN_t$$\n",
    "в которой резервированная цена считается по формуле\n",
    "$$r = S_t -q\\gamma\\sigma^2 (T-t) - \\lambda \\mathbb{E}[e^{-\\gamma J} - 1](T-t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from binance import Client\n",
    "from gym import spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time     open     high      low    close     volume  trades\n",
      "0 2025-03-17 14:38:00  1902.73  1906.57  1902.64  1904.84   312.7654  2639.0\n",
      "1 2025-03-17 14:39:00  1904.84  1907.83  1904.84  1905.86   489.3576  2253.0\n",
      "2 2025-03-17 14:40:00  1905.86  1906.87  1904.34  1904.87  1222.1563  2289.0\n",
      "3 2025-03-17 14:41:00  1904.88  1904.96  1902.65  1903.61  1085.9549  2013.0\n",
      "4 2025-03-17 14:42:00  1903.61  1904.37  1903.36  1903.95   177.1586  1032.0\n"
     ]
    }
   ],
   "source": [
    "# Инициализация клиента (без API ключей для публичных данных)\n",
    "client = Client()\n",
    "\n",
    "\n",
    "# Загрузка исторических данных\n",
    "def get_klines(\n",
    "    symbol=\"ETHUSDT\",\n",
    "    interval=\"1m\",\n",
    "    start_date=\"6 day ago UTC\",\n",
    "    end_date=\"2 day ago UTC\",\n",
    "):\n",
    "    try:\n",
    "        # Получение данных\n",
    "        klines = client.get_historical_klines(\n",
    "            symbol=symbol, interval=interval, start_str=start_date, end_str=end_date\n",
    "        )\n",
    "\n",
    "        # Преобразование в DataFrame\n",
    "        columns = [\n",
    "            \"time\",\n",
    "            \"open\",\n",
    "            \"high\",\n",
    "            \"low\",\n",
    "            \"close\",\n",
    "            \"volume\",\n",
    "            \"close_time\",\n",
    "            \"quote_volume\",\n",
    "            \"trades\",\n",
    "            \"taker_buy_base\",\n",
    "            \"taker_buy_quote\",\n",
    "            \"ignore\",\n",
    "        ]\n",
    "        df = pd.DataFrame(klines, columns=columns, dtype=float)\n",
    "\n",
    "        # Конвертация времени в читаемый формат\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"ms\")\n",
    "\n",
    "        return df[[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Получить данные за последние 24 часа (1-минутные, 1- секундные свечи)\n",
    "# df = get_klines(symbol=\"ETHUSDT\", interval=\"1s\")\n",
    "df = get_klines(symbol=\"ETHUSDT\", interval=\"1m\")\n",
    "# Вывод первых 5 строк\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df)\n",
    "ratio = 0.8\n",
    "df_test = df[int(N * ratio) :]\n",
    "df = df[: int(N * ratio)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В этом подходе мы настраиваем параметр гамма с помощью RL для максимизации PnL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Среда\n",
    "- **Действие**: выбор $\\gamma$ из квантовонного набора\n",
    "- **Наблюдение**: inventory, относительное изменение цены (return), PnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketMakingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Окружение маркет-мейкинга по Avellaneda-Stoikov без учета задержки.\n",
    "    Состояние: [inventory, относительное изменение цены, PnL]\n",
    "    Действие: выбор индекса для γ из заданного списка (например, [0.01, 0.1, 0.5, 1.0])\n",
    "\n",
    "    #TODO\n",
    "    - kappa - параметра интенсивности, определяется из книги заказов\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, gamma_values, T=60.0, kappa=1e-3):\n",
    "        super(MarketMakingEnv, self).__init__()\n",
    "        self.prices = df.close.values\n",
    "        self.prices_high = df.high.values\n",
    "        self.prices_low = df.low.values\n",
    "        self.N = len(df)\n",
    "        self.gamma_values = gamma_values\n",
    "        self.T = T  # горизонт торгов (сек)\n",
    "        self.kappa = kappa  # параметр ликвидности\n",
    "        # Определяем пространства:\n",
    "        # Действие – дискретный выбор из len(gamma_values)\n",
    "        self.action_space = spaces.Discrete(len(gamma_values))\n",
    "        # Состояние: [inventory, relative price change, PnL]\n",
    "        high = np.array([100.0, 1.0, 1e9], dtype=np.float32)\n",
    "        low = np.array([-100.0, -1.0, -1e9], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "        self.initial_cash = 10000.0\n",
    "        self.fee_rate = 0.0  # можно задать комиссию\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.inventory = 0.0\n",
    "        self.cash = self.initial_cash\n",
    "        self.prev_price = self.prices[self.t]\n",
    "        # Начальное состояние: инвентарь 0, изменения цены = 0, PnL = 0\n",
    "        obs = np.array([self.inventory, 0.0, 0.0], dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        gamma = self.gamma_values[action]\n",
    "        mid_price = self.prices[self.t]\n",
    "        # Оценка волатильности на окне последних 60 минут\n",
    "        sigma = 0.0\n",
    "        if self.t > 1:\n",
    "            start = max(0, self.t - 60)\n",
    "            window_prices = self.prices[start : self.t + 1]\n",
    "            if len(window_prices) > 1:\n",
    "                returns = np.diff(window_prices) / np.array(window_prices[:-1])\n",
    "                sigma = np.std(returns)\n",
    "        # Вычисляем цену резервирования и ширину спреда\n",
    "        reservation_price = mid_price - self.inventory * gamma * (sigma**2) * self.T\n",
    "        delta = gamma * (sigma**2) * self.T + (1.0 / gamma) * math.log(\n",
    "            1 + gamma / self.kappa\n",
    "        )\n",
    "\n",
    "        bid_price = reservation_price - delta / 2\n",
    "        ask_price = reservation_price + delta / 2\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "        if self.t < self.N - 1:\n",
    "            next_price = self.prices[self.t + 1]\n",
    "            # Если цена выше ask – исполняется заявка на продажу (продажа 1 единицы)\n",
    "            if self.prices_high[self.t + 1] >= ask_price:\n",
    "                self.inventory -= 1.0\n",
    "                self.cash += ask_price * (1.0 - self.fee_rate)\n",
    "            # Если цена ниже bid – исполняется заявка на покупку (покупка 1 единицы)\n",
    "            if self.prices_low[self.t + 1] <= bid_price:\n",
    "                self.inventory += 1.0\n",
    "                self.cash -= bid_price * (1.0 + self.fee_rate)\n",
    "            current_value = self.cash + self.inventory * next_price\n",
    "            prev_value = self.cash + self.inventory * mid_price\n",
    "            pnl_change = current_value - prev_value\n",
    "            # Небольшой штраф за большую позицию\n",
    "            reward = pnl_change - 0.001 * abs(self.inventory)\n",
    "            self.t += 1\n",
    "        else:\n",
    "            done = True\n",
    "            current_value = self.cash + self.inventory * mid_price\n",
    "            reward = current_value - self.initial_cash\n",
    "        if not done:\n",
    "            new_mid = self.prices[self.t]\n",
    "            price_change = (new_mid - mid_price) / mid_price\n",
    "            pnl = self.cash + self.inventory * new_mid - self.initial_cash\n",
    "            obs = np.array([self.inventory, price_change, pnl], dtype=np.float32)\n",
    "        else:\n",
    "            obs = None\n",
    "        return obs, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Агенты\n",
    "Используем DQN и A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# DQN-агент\n",
    "\n",
    "# Определяем простую Q-сеть\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "Transition = namedtuple(\n",
    "    \"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        buffer_capacity=10000,\n",
    "        batch_size=64,\n",
    "        target_update=1000,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.steps_done = 0\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def push_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.FloatTensor(batch.state).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).unsqueeze(1).to(self.device)\n",
    "        non_final_mask = torch.tensor(\n",
    "            [s is not None for s in batch.next_state],\n",
    "            dtype=torch.bool,\n",
    "            device=self.device,\n",
    "        )\n",
    "        non_final_next_states = torch.FloatTensor(\n",
    "            [s for s in batch.next_state if s is not None]\n",
    "        ).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch.done).unsqueeze(1).to(self.device)\n",
    "        # Q(s,a)\n",
    "        q_values = self.q_network(state_batch).gather(1, action_batch)\n",
    "        # Q_target\n",
    "        next_q_values = torch.zeros(self.batch_size, 1).to(self.device)\n",
    "        if non_final_next_states.size(0) > 0:\n",
    "            next_q_values[non_final_mask] = self.target_network(\n",
    "                non_final_next_states\n",
    "            ).max(1, keepdim=True)[0]\n",
    "        expected_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
    "        loss = nn.MSELoss()(q_values, expected_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.steps_done += 1\n",
    "        if self.steps_done % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# A2C-агент\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.actor = nn.Linear(64, action_dim)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ActorCritic(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, value = self.model(state_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        action = torch.multinomial(probs, num_samples=1)\n",
    "        log_prob = torch.log(probs.gather(1, action))\n",
    "        return action.item(), log_prob.squeeze(), value.squeeze(), logits.squeeze()\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        # trajectories: список кортежей (state, action, log_prob, value, reward, done, logits)\n",
    "\n",
    "        log_probs = torch.stack([t[2] for t in trajectories]).to(self.device)\n",
    "        values = torch.stack([t[3] for t in trajectories]).to(self.device)\n",
    "        rewards = [t[4] for t in trajectories]\n",
    "        dones = [t[5] for t in trajectories]\n",
    "        logits_list = torch.stack([t[6] for t in trajectories]).to(self.device)\n",
    "\n",
    "        # Вычисляем кумулятивную сумму вознаграждений (returns) обратным проходом\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = returns - values\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        probs = torch.softmax(logits_list, dim=1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение DQN-агента...\n",
      "Эпизод 1/20: суммарное вознаграждение = 338.21\n",
      "Эпизод 2/20: суммарное вознаграждение = 173.55\n",
      "Эпизод 3/20: суммарное вознаграждение = 57.82\n",
      "Эпизод 4/20: суммарное вознаграждение = 873.25\n",
      "Эпизод 5/20: суммарное вознаграждение = 1368.27\n",
      "Эпизод 6/20: суммарное вознаграждение = 22.46\n",
      "Эпизод 7/20: суммарное вознаграждение = -779.97\n",
      "Эпизод 8/20: суммарное вознаграждение = -169.03\n",
      "Эпизод 9/20: суммарное вознаграждение = -102.55\n",
      "Эпизод 10/20: суммарное вознаграждение = 866.24\n",
      "Эпизод 11/20: суммарное вознаграждение = -363.19\n",
      "Эпизод 12/20: суммарное вознаграждение = 495.31\n",
      "Эпизод 13/20: суммарное вознаграждение = 470.04\n",
      "Эпизод 14/20: суммарное вознаграждение = 837.97\n",
      "Эпизод 15/20: суммарное вознаграждение = 294.30\n",
      "Эпизод 16/20: суммарное вознаграждение = -575.31\n",
      "Эпизод 17/20: суммарное вознаграждение = -243.46\n",
      "Эпизод 18/20: суммарное вознаграждение = -409.54\n",
      "Эпизод 19/20: суммарное вознаграждение = -174.26\n",
      "Эпизод 20/20: суммарное вознаграждение = 425.28\n",
      "\n",
      "Обучение A2C-агента...\n",
      "Эпизод 1/20: суммарное вознаграждение = 997.01\n",
      "Эпизод 2/20: суммарное вознаграждение = -185.62\n",
      "Эпизод 3/20: суммарное вознаграждение = -52.45\n",
      "Эпизод 4/20: суммарное вознаграждение = 804.64\n",
      "Эпизод 5/20: суммарное вознаграждение = 286.77\n",
      "Эпизод 6/20: суммарное вознаграждение = -46.30\n",
      "Эпизод 7/20: суммарное вознаграждение = -426.55\n",
      "Эпизод 8/20: суммарное вознаграждение = 619.37\n",
      "Эпизод 9/20: суммарное вознаграждение = 458.50\n",
      "Эпизод 10/20: суммарное вознаграждение = 829.79\n",
      "Эпизод 11/20: суммарное вознаграждение = -347.17\n",
      "Эпизод 12/20: суммарное вознаграждение = 168.72\n",
      "Эпизод 13/20: суммарное вознаграждение = 605.20\n",
      "Эпизод 14/20: суммарное вознаграждение = 465.81\n",
      "Эпизод 15/20: суммарное вознаграждение = -133.56\n",
      "Эпизод 16/20: суммарное вознаграждение = 93.98\n",
      "Эпизод 17/20: суммарное вознаграждение = 318.99\n",
      "Эпизод 18/20: суммарное вознаграждение = 812.98\n",
      "Эпизод 19/20: суммарное вознаграждение = 441.92\n",
      "Эпизод 20/20: суммарное вознаграждение = -59.84\n"
     ]
    }
   ],
   "source": [
    "# Задаём параметры окружения и агентов\n",
    "gamma_values = [0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "env = MarketMakingEnv(df=df, gamma_values=gamma_values, T=60.0, kappa=1e-3)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Параметры для обучения DQN\n",
    "dqn_agent = DQNAgent(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    buffer_capacity=10000,\n",
    "    batch_size=64,\n",
    "    target_update=1000,\n",
    ")\n",
    "\n",
    "\n",
    "num_episodes = 20\n",
    "num_episodes_a2c = 20\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 300\n",
    "\n",
    "print(\"Обучение DQN-агента...\")\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # ε-жадная политика\n",
    "        epsilon = epsilon_final + (epsilon_start - epsilon_final) * math.exp(\n",
    "            -env.t / epsilon_decay\n",
    "        )\n",
    "        action = dqn_agent.select_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        dqn_agent.push_transition(state, action, reward, next_state, done)\n",
    "        dqn_agent.update()\n",
    "        state = next_state if next_state is not None else state\n",
    "        total_reward += reward\n",
    "    print(\n",
    "        f\"Эпизод {episode+1}/{num_episodes}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "# Обучение A2C-агента\n",
    "\n",
    "a2c_agent = A2CAgent(\n",
    "    state_dim, action_dim, lr=1e-3, gamma=0.99, value_coef=0.5, entropy_coef=0.01\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nОбучение A2C-агента...\")\n",
    "\n",
    "rollout_length = 10  # собираем траектории по 10 шагов\n",
    "for episode in range(num_episodes_a2c):\n",
    "    state = env.reset()\n",
    "    trajectories = []\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Собираем траекторию rollout_length\n",
    "        for _ in range(rollout_length):\n",
    "            action, log_prob, value, logits = a2c_agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectories.append(\n",
    "                (state, action, log_prob, value, reward, float(done), logits)\n",
    "            )\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        a2c_agent.update(trajectories)\n",
    "        trajectories = []\n",
    "    print(\n",
    "        f\"Эпизод {episode+1}/{num_episodes_a2c}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(env, agent, agent_type=\"dqn\"):\n",
    "    state = env.reset()\n",
    "    pnl_history = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        if agent_type == \"dqn\":\n",
    "            action = agent.select_action(state, epsilon=0.0)\n",
    "        elif agent_type == \"a2c\":\n",
    "            action, _, _, _ = agent.select_action(state)\n",
    "        else:\n",
    "            action = 0  # базовая стратегия\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        # Третий элемент состояния – PnL\n",
    "        if state is not None:\n",
    "            pnl = state[2]\n",
    "            pnl_history.append(pnl)\n",
    "        state = next_state if next_state is not None else state\n",
    "    return pnl_history\n",
    "\n",
    "\n",
    "# Прогон теста для обоих агентов\n",
    "env_test = MarketMakingEnv(df=df_test, gamma_values=gamma_values, T=60.0, kappa=1e-3)\n",
    "dqn_pnl = run_agent(env_test, dqn_agent, agent_type=\"dqn\")\n",
    "env_test = MarketMakingEnv(df=df_test, gamma_values=gamma_values, T=60.0, kappa=1e-3)\n",
    "a2c_pnl = run_agent(env_test, a2c_agent, agent_type=\"a2c\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты тестирования:\n",
      "DQN итоговый PnL: 0.00, Max Drawdown: 0.00, Sharpe Ratio: 0.00\n",
      "A2C итоговый PnL: -8.07, Max Drawdown: -52.28, Sharpe Ratio: -0.18\n"
     ]
    }
   ],
   "source": [
    "def compute_max_drawdown(pnl_history):\n",
    "    pnl_array = np.array(pnl_history)\n",
    "    running_max = np.maximum.accumulate(pnl_array)\n",
    "    drawdowns = pnl_array - running_max\n",
    "    max_drawdown = drawdowns.min()\n",
    "    return max_drawdown\n",
    "\n",
    "\n",
    "def compute_sharpe_ratio(pnl_history):\n",
    "    pnl_array = np.array(pnl_history)\n",
    "    returns = np.diff(pnl_array)\n",
    "    if returns.std() == 0:\n",
    "        return 0.0\n",
    "    sharpe = returns.mean() / returns.std() * np.sqrt(len(returns))\n",
    "    return sharpe\n",
    "\n",
    "\n",
    "dqn_max_dd = compute_max_drawdown(dqn_pnl)\n",
    "dqn_sharpe = compute_sharpe_ratio(dqn_pnl)\n",
    "a2c_max_dd = compute_max_drawdown(a2c_pnl)\n",
    "a2c_sharpe = compute_sharpe_ratio(a2c_pnl)\n",
    "\n",
    "print(\"\\nРезультаты тестирования:\")\n",
    "print(\n",
    "    f\"DQN итоговый PnL: {dqn_pnl[-1]:.2f}, Max Drawdown: {dqn_max_dd:.2f}, Sharpe Ratio: {dqn_sharpe:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"A2C итоговый PnL: {a2c_pnl[-1]:.2f}, Max Drawdown: {a2c_max_dd:.2f}, Sharpe Ratio: {a2c_sharpe:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:  \n",
    "С задержкой и без задержки алгоритмы показали примерно одинаковые, близкие к случайным  результаты  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
