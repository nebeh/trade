{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from binance import Client\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные binance ETHUSDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time     open     high      low    close    volume  trades\n",
      "0 2025-03-17 16:10:00  1923.49  1924.36  1922.03  1922.17  376.2700  1728.0\n",
      "1 2025-03-17 16:11:00  1922.16  1922.95  1920.21  1920.70  173.0118  1689.0\n",
      "2 2025-03-17 16:12:00  1920.70  1921.72  1919.69  1920.41  265.8190  1691.0\n",
      "3 2025-03-17 16:13:00  1920.42  1921.84  1920.01  1920.70  418.7001  1556.0\n",
      "4 2025-03-17 16:14:00  1920.71  1921.64  1920.00  1920.46  271.7455  1210.0\n"
     ]
    }
   ],
   "source": [
    "# Инициализация клиента Binance (публичный доступ)\n",
    "client = Client()\n",
    "\n",
    "\n",
    "# Загрузка исторических данных (свечи)\n",
    "def get_klines(\n",
    "    symbol=\"ETHUSDT\",\n",
    "    interval=\"1m\",\n",
    "    start_date=\"6 day ago UTC\",\n",
    "    end_date=\"2 day ago UTC\",\n",
    "):\n",
    "    try:\n",
    "        klines = client.get_historical_klines(\n",
    "            symbol=symbol, interval=interval, start_str=start_date, end_str=end_date\n",
    "        )\n",
    "        columns = [\n",
    "            \"time\",\n",
    "            \"open\",\n",
    "            \"high\",\n",
    "            \"low\",\n",
    "            \"close\",\n",
    "            \"volume\",\n",
    "            \"close_time\",\n",
    "            \"quote_volume\",\n",
    "            \"trades\",\n",
    "            \"taker_buy_base\",\n",
    "            \"taker_buy_quote\",\n",
    "            \"ignore\",\n",
    "        ]\n",
    "        df = pd.DataFrame(klines, columns=columns, dtype=float)\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"ms\")\n",
    "        return df[[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при получении данных: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "df = get_klines(symbol=\"ETHUSDT\", interval=\"1m\")\n",
    "print(df.head())\n",
    "\n",
    "# Разбиваем данные на обучающую и тестовую выборки\n",
    "if df is not None:\n",
    "    N = len(df)\n",
    "    ratio = 0.8\n",
    "    df_test = df[int(N * ratio) :].reset_index(drop=True)\n",
    "    df = df[: int(N * ratio)].reset_index(drop=True)\n",
    "else:\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]\n",
    "    )\n",
    "    df_test = pd.DataFrame(\n",
    "        columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Среда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketMakingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Окружение маркет-мейкинга по Avellaneda-Stoikov с использованием свечных данных.\n",
    "    Состояние: [inventory, history(relative price changes, длина=16), PnL].\n",
    "    Действие: мультидискретный выбор (γ, κ, Δσ), где:\n",
    "      - γ: коэффициент аверсии к риску,\n",
    "      - κ: интенсивность ликвидности,\n",
    "      - Δσ: поправка к оценённой волатильности.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, df, gamma_values, kappa_values, vol_corr_values, T=60.0, history_len=16\n",
    "    ):\n",
    "        super(MarketMakingEnv, self).__init__()\n",
    "        self.prices = df.close.values\n",
    "        self.prices_high = df.high.values\n",
    "        self.prices_low = df.low.values\n",
    "        self.N = len(df)\n",
    "        self.gamma_values = gamma_values\n",
    "        self.kappa_values = kappa_values\n",
    "        self.vol_corr_values = vol_corr_values\n",
    "        self.T = T  # торговый горизонт (сек) для расчёта оптимальных котировок\n",
    "        self.history_len = history_len\n",
    "        # Пространство действий: мультидискретное – [#γ, #κ, #Δσ]\n",
    "        self.action_space = spaces.MultiDiscrete(\n",
    "            [len(gamma_values), len(kappa_values), len(vol_corr_values)]\n",
    "        )\n",
    "        # Пространство состояний: размер = 1 (inventory) + history_len (относительные изменения цены) + 1 (PnL)\n",
    "        low = np.concatenate(\n",
    "            (np.array([-100.0]), np.full((history_len,), -1.0), np.array([-1e9]))\n",
    "        )\n",
    "        high = np.concatenate(\n",
    "            (np.array([100.0]), np.full((history_len,), 1.0), np.array([1e9]))\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "        self.initial_cash = 10000.0\n",
    "        self.fee_rate = 0.0  # комиссия\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.inventory = 0.0\n",
    "        self.cash = self.initial_cash\n",
    "        # Инициализируем историю относительных изменений цены нулями\n",
    "        self.price_change_history = [0.0] * self.history_len\n",
    "        # Начальное наблюдение: [inventory, history, pnl]\n",
    "        pnl = self.cash - self.initial_cash\n",
    "        state = np.concatenate(\n",
    "            ([self.inventory], np.array(self.price_change_history), [pnl])\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Раскодирование мультидискретного действия:\n",
    "        n_gamma = len(self.gamma_values)\n",
    "        n_kappa = len(self.kappa_values)\n",
    "        n_vol = len(self.vol_corr_values)\n",
    "        if isinstance(action, (np.ndarray, list, tuple)):\n",
    "            gamma_idx = int(action[0])\n",
    "            kappa_idx = int(action[1])\n",
    "            vol_corr_idx = int(action[2])\n",
    "        else:\n",
    "            action = int(action)\n",
    "            gamma_idx = action // (n_gamma * n_vol)\n",
    "            rem = action % (n_kappa * n_vol)\n",
    "            kappa_idx = rem // n_vol\n",
    "            vol_corr_idx = rem % n_vol\n",
    "\n",
    "        gamma = self.gamma_values[gamma_idx]\n",
    "        kappa = self.kappa_values[kappa_idx]\n",
    "        vol_corr = self.vol_corr_values[vol_corr_idx]\n",
    "\n",
    "        mid_price = self.prices[self.t]\n",
    "        # Оценка волатильности на окне длиной T секунд\n",
    "        sigma_est = 0.0\n",
    "        if self.t > 1:\n",
    "            start = max(0, self.t - int(self.T))\n",
    "            window_prices = self.prices[start : self.t + 1]\n",
    "            if len(window_prices) > 1:\n",
    "                returns = np.diff(window_prices) / window_prices[:-1]\n",
    "                sigma_est = np.std(returns)\n",
    "        effective_sigma = sigma_est * vol_corr\n",
    "        if effective_sigma < 1e-8:\n",
    "            effective_sigma = 1e-8\n",
    "\n",
    "        # Расчет цены резервирования и оптимального спреда\n",
    "        reservation_price = (\n",
    "            mid_price - self.inventory * gamma * (effective_sigma**2) * self.T\n",
    "        )\n",
    "        delta = gamma * (effective_sigma**2) * self.T + (1.0 / gamma) * math.log(\n",
    "            1 + gamma / kappa\n",
    "        )\n",
    "        bid_price = reservation_price - delta / 2\n",
    "        ask_price = reservation_price + delta / 2\n",
    "\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "        if self.t < self.N - 1:\n",
    "            next_price = self.prices[self.t + 1]\n",
    "            if self.prices_high[self.t + 1] >= ask_price:\n",
    "                self.inventory -= 1.0\n",
    "                self.cash += ask_price * (1.0 - self.fee_rate)\n",
    "            if self.prices_low[self.t + 1] <= bid_price:\n",
    "                self.inventory += 1.0\n",
    "                self.cash -= bid_price * (1.0 + self.fee_rate)\n",
    "            current_value = self.cash + self.inventory * next_price\n",
    "            prev_value = self.cash + self.inventory * mid_price\n",
    "            pnl_change = current_value - prev_value\n",
    "            reward = pnl_change - 0.001 * abs(self.inventory)\n",
    "            self.t += 1\n",
    "        else:\n",
    "            done = True\n",
    "            current_value = self.cash + self.inventory * mid_price\n",
    "            reward = current_value - self.initial_cash\n",
    "\n",
    "        # Обновляем историю относительных изменений цены:\n",
    "        if self.t > 0:\n",
    "            # Рассчитываем относительное изменение цены между предыдущей и текущей свечами\n",
    "            rel_change = (self.prices[self.t] - self.prices[self.t - 1]) / self.prices[\n",
    "                self.t - 1\n",
    "            ]\n",
    "        else:\n",
    "            rel_change = 0.0\n",
    "        # Сдвигаем историю (удаляем самый старый элемент и добавляем новый)\n",
    "        self.price_change_history.pop(0)\n",
    "        self.price_change_history.append(rel_change)\n",
    "\n",
    "        if not done:\n",
    "            new_mid = self.prices[self.t]\n",
    "            pnl = self.cash + self.inventory * new_mid - self.initial_cash\n",
    "            # Собираем наблюдение: [inventory, history(16 значений), pnl]\n",
    "            obs = np.concatenate(\n",
    "                ([self.inventory], np.array(self.price_change_history), [pnl])\n",
    "            ).astype(np.float32)\n",
    "        else:\n",
    "            obs = None\n",
    "        return obs, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Агенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# DQN-агент\n",
    "###############################################################################\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "Transition = namedtuple(\n",
    "    \"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        buffer_capacity=10000,\n",
    "        batch_size=64,\n",
    "        target_update=1000,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.steps_done = 0\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def push_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.FloatTensor(batch.state).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).unsqueeze(1).to(self.device)\n",
    "        non_final_mask = torch.tensor(\n",
    "            [s is not None for s in batch.next_state],\n",
    "            dtype=torch.bool,\n",
    "            device=self.device,\n",
    "        )\n",
    "        non_final_next_states = torch.FloatTensor(\n",
    "            [s for s in batch.next_state if s is not None]\n",
    "        ).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch.done).unsqueeze(1).to(self.device)\n",
    "        q_values = self.q_network(state_batch).gather(1, action_batch)\n",
    "        next_q_values = torch.zeros(self.batch_size, 1).to(self.device)\n",
    "        if non_final_next_states.size(0) > 0:\n",
    "            next_q_values[non_final_mask] = self.target_network(\n",
    "                non_final_next_states\n",
    "            ).max(1, keepdim=True)[0]\n",
    "        expected_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
    "        loss = nn.MSELoss()(q_values, expected_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.steps_done += 1\n",
    "        if self.steps_done % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# A2C-агент\n",
    "###############################################################################\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.actor = nn.Linear(64, action_dim)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ActorCritic(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, value = self.model(state_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        action_tensor = torch.multinomial(probs, num_samples=1)\n",
    "        log_prob = torch.log(probs.gather(1, action_tensor) + 1e-10)\n",
    "        return (\n",
    "            action_tensor.item(),\n",
    "            log_prob.squeeze(),\n",
    "            value.squeeze(),\n",
    "            logits.squeeze(),\n",
    "        )\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        log_probs = torch.stack([t[2] for t in trajectories]).to(self.device)\n",
    "        values = torch.stack([t[3] for t in trajectories]).to(self.device)\n",
    "        rewards = [t[4] for t in trajectories]\n",
    "        dones = [t[5] for t in trajectories]\n",
    "        logits_list = torch.stack([t[6] for t in trajectories]).to(self.device)\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = returns - values\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        probs = torch.softmax(logits_list, dim=1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# PPO-агент\n",
    "###############################################################################\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        clip_coef=0.2,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "        ppo_epochs=4,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ActorCritic(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_coef = clip_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, value = self.model(state_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        action_tensor = torch.multinomial(probs, num_samples=1)\n",
    "        log_prob = torch.log(probs.gather(1, action_tensor) + 1e-10)\n",
    "        return action_tensor.item(), log_prob.squeeze(), value.squeeze()\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        states = torch.FloatTensor([t[0] for t in trajectories]).to(self.device)\n",
    "        actions = (\n",
    "            torch.LongTensor([t[1] for t in trajectories]).unsqueeze(1).to(self.device)\n",
    "        )\n",
    "        old_log_probs = torch.stack([t[2] for t in trajectories]).to(self.device)\n",
    "        old_values = torch.stack([t[3] for t in trajectories]).to(self.device)\n",
    "        rewards = [t[4] for t in trajectories]\n",
    "        dones = [t[5] for t in trajectories]\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        old_log_probs = old_log_probs.detach()\n",
    "        old_values = old_values.detach()\n",
    "        advantages = returns - old_values\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            logits, values = self.model(states)\n",
    "            values = values.squeeze(1)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            new_log_probs = torch.log(probs.gather(1, actions) + 1e-10).squeeze(1)\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            adv_detached = advantages.detach()\n",
    "            surr1 = ratio * adv_detached\n",
    "            surr2 = (\n",
    "                torch.clamp(ratio, 1.0 - self.clip_coef, 1.0 + self.clip_coef)\n",
    "                * adv_detached\n",
    "            )\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (returns - values).pow(2).mean()\n",
    "            entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "            loss = (\n",
    "                actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "            )\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strike/work/penv/deep/lib/python3.13/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Функции для вычисления метрик: максимальная просадка и коэффициент Шарпа\n",
    "###############################################################################\n",
    "def compute_max_drawdown(pnl_history):\n",
    "    pnl_array = np.array(pnl_history)\n",
    "    running_max = np.maximum.accumulate(pnl_array)\n",
    "    drawdowns = pnl_array - running_max\n",
    "    max_drawdown = drawdowns.min()\n",
    "    return max_drawdown\n",
    "\n",
    "\n",
    "def compute_sharpe_ratio(pnl_history):\n",
    "    pnl_array = np.array(pnl_history)\n",
    "    returns = np.diff(pnl_array)\n",
    "    if returns.std() == 0:\n",
    "        return 0.0\n",
    "    sharpe = returns.mean() / returns.std() * np.sqrt(len(returns))\n",
    "    return sharpe\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Определяем дискретные наборы параметров для оптимизации\n",
    "###############################################################################\n",
    "gamma_values = [0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "kappa_values = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
    "vol_corr_values = [0.5, -0.75, 1.0, 0.25, 0.5]  # поправка к sigma\n",
    "\n",
    "# Создаем окружение с новым мультидискретным пространством действий\n",
    "env = MarketMakingEnv(\n",
    "    df=df,\n",
    "    gamma_values=gamma_values,\n",
    "    kappa_values=kappa_values,\n",
    "    vol_corr_values=vol_corr_values,\n",
    "    T=60.0,\n",
    "    history_len=16,\n",
    ")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "if isinstance(env.action_space, spaces.MultiDiscrete):\n",
    "    action_dim = int(np.prod(env.action_space.nvec))\n",
    "else:\n",
    "    action_dim = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение DQN-агента...\n",
      "DQN Эпизод 1/20: суммарное вознаграждение = 180.33\n",
      "DQN Эпизод 2/20: суммарное вознаграждение = -40.50\n",
      "DQN Эпизод 3/20: суммарное вознаграждение = 160.91\n",
      "DQN Эпизод 4/20: суммарное вознаграждение = 363.26\n",
      "DQN Эпизод 5/20: суммарное вознаграждение = 80.09\n",
      "DQN Эпизод 6/20: суммарное вознаграждение = -56.10\n",
      "DQN Эпизод 7/20: суммарное вознаграждение = 38.27\n",
      "DQN Эпизод 8/20: суммарное вознаграждение = -198.75\n",
      "DQN Эпизод 9/20: суммарное вознаграждение = 355.97\n",
      "DQN Эпизод 10/20: суммарное вознаграждение = 1046.52\n",
      "DQN Эпизод 11/20: суммарное вознаграждение = 466.70\n",
      "DQN Эпизод 12/20: суммарное вознаграждение = 437.95\n",
      "DQN Эпизод 13/20: суммарное вознаграждение = 995.75\n",
      "DQN Эпизод 14/20: суммарное вознаграждение = -150.07\n",
      "DQN Эпизод 15/20: суммарное вознаграждение = 493.17\n",
      "DQN Эпизод 16/20: суммарное вознаграждение = 3.27\n",
      "DQN Эпизод 17/20: суммарное вознаграждение = 883.49\n",
      "DQN Эпизод 18/20: суммарное вознаграждение = -413.00\n",
      "DQN Эпизод 19/20: суммарное вознаграждение = -226.09\n",
      "DQN Эпизод 20/20: суммарное вознаграждение = 178.83\n",
      "\n",
      "Обучение A2C-агента...\n",
      "A2C Эпизод 1/20: суммарное вознаграждение = -133.67\n",
      "A2C Эпизод 2/20: суммарное вознаграждение = -127.34\n",
      "A2C Эпизод 3/20: суммарное вознаграждение = 130.72\n",
      "A2C Эпизод 4/20: суммарное вознаграждение = 353.20\n",
      "A2C Эпизод 5/20: суммарное вознаграждение = -185.25\n",
      "A2C Эпизод 6/20: суммарное вознаграждение = 371.79\n",
      "A2C Эпизод 7/20: суммарное вознаграждение = 478.04\n",
      "A2C Эпизод 8/20: суммарное вознаграждение = 373.70\n",
      "A2C Эпизод 9/20: суммарное вознаграждение = 602.95\n",
      "A2C Эпизод 10/20: суммарное вознаграждение = -46.04\n",
      "A2C Эпизод 11/20: суммарное вознаграждение = 83.85\n",
      "A2C Эпизод 12/20: суммарное вознаграждение = 319.68\n",
      "A2C Эпизод 13/20: суммарное вознаграждение = -76.92\n",
      "A2C Эпизод 14/20: суммарное вознаграждение = 178.13\n",
      "A2C Эпизод 15/20: суммарное вознаграждение = 382.30\n",
      "A2C Эпизод 16/20: суммарное вознаграждение = 225.45\n",
      "A2C Эпизод 17/20: суммарное вознаграждение = -140.55\n",
      "A2C Эпизод 18/20: суммарное вознаграждение = 361.36\n",
      "A2C Эпизод 19/20: суммарное вознаграждение = 347.60\n",
      "A2C Эпизод 20/20: суммарное вознаграждение = 510.77\n",
      "\n",
      "Обучение PPO-агента...\n",
      "PPO Эпизод 1/20: суммарное вознаграждение = 837.38\n",
      "PPO Эпизод 2/20: суммарное вознаграждение = 531.75\n",
      "PPO Эпизод 3/20: суммарное вознаграждение = 524.61\n",
      "PPO Эпизод 4/20: суммарное вознаграждение = 483.58\n",
      "PPO Эпизод 5/20: суммарное вознаграждение = 368.51\n",
      "PPO Эпизод 6/20: суммарное вознаграждение = 1038.76\n",
      "PPO Эпизод 7/20: суммарное вознаграждение = 315.10\n",
      "PPO Эпизод 8/20: суммарное вознаграждение = 141.73\n",
      "PPO Эпизод 9/20: суммарное вознаграждение = 252.66\n",
      "PPO Эпизод 10/20: суммарное вознаграждение = 406.96\n",
      "PPO Эпизод 11/20: суммарное вознаграждение = -8.93\n",
      "PPO Эпизод 12/20: суммарное вознаграждение = -166.53\n",
      "PPO Эпизод 13/20: суммарное вознаграждение = -17.95\n",
      "PPO Эпизод 14/20: суммарное вознаграждение = 361.79\n",
      "PPO Эпизод 15/20: суммарное вознаграждение = -764.20\n",
      "PPO Эпизод 16/20: суммарное вознаграждение = 551.40\n",
      "PPO Эпизод 17/20: суммарное вознаграждение = 1699.69\n",
      "PPO Эпизод 18/20: суммарное вознаграждение = -38.76\n",
      "PPO Эпизод 19/20: суммарное вознаграждение = -388.49\n",
      "PPO Эпизод 20/20: суммарное вознаграждение = 208.24\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Инициализируем агентов DQN, A2C и PPO\n",
    "###############################################################################\n",
    "\n",
    "dqn_agent = DQNAgent(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    buffer_capacity=10000,\n",
    "    batch_size=64,\n",
    "    target_update=1000,\n",
    ")\n",
    "a2c_agent = A2CAgent(\n",
    "    state_dim, action_dim, lr=1e-3, gamma=0.99, value_coef=0.5, entropy_coef=0.01\n",
    ")\n",
    "ppo_agent = PPOAgent(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    clip_coef=0.2,\n",
    "    value_coef=0.5,\n",
    "    entropy_coef=0.01,\n",
    "    ppo_epochs=4,\n",
    ")\n",
    "\n",
    "###############################################################################\n",
    "# Параметры обучения\n",
    "###############################################################################\n",
    "num_episodes = 20  # для DQN\n",
    "num_episodes_a2c = 20  # для A2C\n",
    "num_episodes_ppo = 20  # для PPO\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 300\n",
    "\n",
    "###############################################################################\n",
    "# Обучение DQN-агента\n",
    "###############################################################################\n",
    "print(\"Обучение DQN-агента...\")\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        epsilon = epsilon_final + (epsilon_start - epsilon_final) * math.exp(\n",
    "            -env.t / epsilon_decay\n",
    "        )\n",
    "        action = dqn_agent.select_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        dqn_agent.push_transition(state, action, reward, next_state, done)\n",
    "        dqn_agent.update()\n",
    "        state = next_state if next_state is not None else state\n",
    "        total_reward += reward\n",
    "    print(\n",
    "        f\"DQN Эпизод {episode+1}/{num_episodes}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "###############################################################################\n",
    "# Обучение A2C-агента\n",
    "###############################################################################\n",
    "print(\"\\nОбучение A2C-агента...\")\n",
    "rollout_length = 10\n",
    "for episode in range(num_episodes_a2c):\n",
    "    state = env.reset()\n",
    "    trajectories = []\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        for _ in range(rollout_length):\n",
    "            action, log_prob, value, logits = a2c_agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectories.append(\n",
    "                (state, action, log_prob, value, reward, float(done), logits)\n",
    "            )\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        a2c_agent.update(trajectories)\n",
    "        trajectories = []\n",
    "    print(\n",
    "        f\"A2C Эпизод {episode+1}/{num_episodes_a2c}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "###############################################################################\n",
    "# Обучение PPO-агента\n",
    "###############################################################################\n",
    "print(\"\\nОбучение PPO-агента...\")\n",
    "rollout_length_ppo = 20\n",
    "for episode in range(num_episodes_ppo):\n",
    "    state = env.reset()\n",
    "    trajectories = []\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        for _ in range(rollout_length_ppo):\n",
    "            action, log_prob, value = ppo_agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectories.append((state, action, log_prob, value, reward, float(done)))\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        ppo_agent.update(trajectories)\n",
    "        trajectories = []\n",
    "    print(\n",
    "        f\"PPO Эпизод {episode+1}/{num_episodes_ppo}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты тестирования:\n",
      "DQN итоговый PnL: 0.00, Max Drawdown: 0.00, Sharpe Ratio: 0.00\n",
      "A2C итоговый PnL: 33.78, Max Drawdown: -19.09, Sharpe Ratio: 0.80\n",
      "PPO итоговый PnL: 33.78, Max Drawdown: -19.09, Sharpe Ratio: 0.80\n"
     ]
    }
   ],
   "source": [
    "def run_agent(env, agent, agent_type=\"dqn\"):\n",
    "    state = env.reset()\n",
    "    pnl_history = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        if agent_type == \"dqn\":\n",
    "            action = agent.select_action(state, epsilon=0.0)\n",
    "        elif agent_type == \"a2c\":\n",
    "            action, _, _, _ = agent.select_action(state)\n",
    "        elif agent_type == \"ppo\":\n",
    "            action, _, _ = agent.select_action(state)\n",
    "        else:\n",
    "            action = 0\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        if state is not None:\n",
    "            pnl_history.append(state[-1])  # последний элемент состояния – PnL\n",
    "        state = next_state if next_state is not None else state\n",
    "    return pnl_history\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Тестирование агентов на отложенной выборке и вычисление метрик\n",
    "###############################################################################\n",
    "env_test = MarketMakingEnv(\n",
    "    df=df_test,\n",
    "    gamma_values=gamma_values,\n",
    "    kappa_values=kappa_values,\n",
    "    vol_corr_values=vol_corr_values,\n",
    "    T=60.0,\n",
    "    history_len=16,\n",
    ")\n",
    "dqn_pnl = run_agent(env_test, dqn_agent, agent_type=\"dqn\")\n",
    "env_test = MarketMakingEnv(\n",
    "    df=df_test,\n",
    "    gamma_values=gamma_values,\n",
    "    kappa_values=kappa_values,\n",
    "    vol_corr_values=vol_corr_values,\n",
    "    T=60.0,\n",
    "    history_len=16,\n",
    ")\n",
    "a2c_pnl = run_agent(env_test, a2c_agent, agent_type=\"a2c\")\n",
    "env_test = MarketMakingEnv(\n",
    "    df=df_test,\n",
    "    gamma_values=gamma_values,\n",
    "    kappa_values=kappa_values,\n",
    "    vol_corr_values=vol_corr_values,\n",
    "    T=60.0,\n",
    "    history_len=16,\n",
    ")\n",
    "ppo_pnl = run_agent(env_test, ppo_agent, agent_type=\"ppo\")\n",
    "\n",
    "dqn_max_dd = compute_max_drawdown(dqn_pnl)\n",
    "dqn_sharpe = compute_sharpe_ratio(dqn_pnl)\n",
    "a2c_max_dd = compute_max_drawdown(a2c_pnl)\n",
    "a2c_sharpe = compute_sharpe_ratio(a2c_pnl)\n",
    "ppo_max_dd = compute_max_drawdown(ppo_pnl)\n",
    "ppo_sharpe = compute_sharpe_ratio(ppo_pnl)\n",
    "\n",
    "print(\"\\nРезультаты тестирования:\")\n",
    "print(\n",
    "    f\"DQN итоговый PnL: {dqn_pnl[-1]:.2f}, Max Drawdown: {dqn_max_dd:.2f}, Sharpe Ratio: {dqn_sharpe:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"A2C итоговый PnL: {a2c_pnl[-1]:.2f}, Max Drawdown: {a2c_max_dd:.2f}, Sharpe Ratio: {a2c_sharpe:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"PPO итоговый PnL: {ppo_pnl[-1]:.2f}, Max Drawdown: {ppo_max_dd:.2f}, Sharpe Ratio: {ppo_sharpe:.2f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:  \n",
    "Результаты все еще не внушают оптимизма"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
