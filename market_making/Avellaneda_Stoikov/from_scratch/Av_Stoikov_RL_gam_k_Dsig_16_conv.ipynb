{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from binance import Client\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = Client()  # Публичный клиент\n",
    "\n",
    "def get_klines(symbol=\"ETHUSDT\", interval=\"1m\", start_date=\"6 day ago UTC\", end_date=\"2 day ago UTC\"):\n",
    "    try:\n",
    "        klines = client.get_historical_klines(\n",
    "            symbol=symbol, interval=interval, start_str=start_date, end_str=end_date\n",
    "        )\n",
    "        columns = [\n",
    "            \"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \n",
    "            \"close_time\", \"quote_volume\", \"trades\", \"taker_buy_base\", \"taker_buy_quote\", \"ignore\"\n",
    "        ]\n",
    "        df = pd.DataFrame(klines, columns=columns, dtype=float)\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"ms\")\n",
    "        return df[[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при получении данных: {e}\")\n",
    "        return None\n",
    "\n",
    "df = get_klines(symbol=\"ETHUSDT\", interval=\"1m\")\n",
    "print(df.head())\n",
    "\n",
    "if df is not None:\n",
    "    N = len(df)\n",
    "    ratio = 0.8\n",
    "    df_test = df[int(N * ratio):].reset_index(drop=True)\n",
    "    df = df[: int(N * ratio)].reset_index(drop=True)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"])\n",
    "    df_test = pd.DataFrame(columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Среда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketMakingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Окружение маркет-мейкинга по Avellaneda-Stoikov с использованием свечных данных.\n",
    "    Состояние: [inventory, history(relative price changes, длина=history_len), PnL].\n",
    "    Действие: мультидискретный выбор (γ, κ, Δσ), где:\n",
    "      - γ: коэффициент аверсии к риску,\n",
    "      - κ: интенсивность ликвидности,\n",
    "      - Δσ: поправка к оценённой волатильности.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, gamma_values, kappa_values, vol_corr_values, T=60.0, history_len=16):\n",
    "        super(MarketMakingEnv, self).__init__()\n",
    "        self.prices = df.close.values\n",
    "        self.prices_high = df.high.values\n",
    "        self.prices_low = df.low.values\n",
    "        self.N = len(df)\n",
    "        self.gamma_values = gamma_values\n",
    "        self.kappa_values = kappa_values\n",
    "        self.vol_corr_values = vol_corr_values\n",
    "        self.T = T  # торговый горизонт (сек) для расчёта котировок\n",
    "        self.history_len = history_len\n",
    "        self.action_space = spaces.MultiDiscrete([len(gamma_values), len(kappa_values), len(vol_corr_values)])\n",
    "        # Размер состояния: 1 (inventory) + history_len (16 значений) + 1 (PnL) = 18\n",
    "        low = np.concatenate((np.array([-100.0]), np.full((history_len,), -1.0), np.array([-1e9])))\n",
    "        high = np.concatenate((np.array([100.0]), np.full((history_len,), 1.0), np.array([1e9])))\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "        self.initial_cash = 10000.0\n",
    "        self.fee_rate = 0.0\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.inventory = 0.0\n",
    "        self.cash = self.initial_cash\n",
    "        self.price_change_history = [0.0] * self.history_len\n",
    "        pnl = self.cash - self.initial_cash\n",
    "        state = np.concatenate(([self.inventory], np.array(self.price_change_history), [pnl]))\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Раскодирование действия (MultiDiscrete)\n",
    "        n_gamma = len(self.gamma_values)\n",
    "        n_kappa = len(self.kappa_values)\n",
    "        n_vol = len(self.vol_corr_values)\n",
    "        if isinstance(action, (np.ndarray, list, tuple)):\n",
    "            gamma_idx = int(action[0])\n",
    "            kappa_idx = int(action[1])\n",
    "            vol_corr_idx = int(action[2])\n",
    "        else:\n",
    "            action = int(action)\n",
    "            gamma_idx = action // (n_gamma * n_vol)\n",
    "            rem = action % (n_kappa * n_vol)\n",
    "            kappa_idx = rem // n_vol\n",
    "            vol_corr_idx = rem % n_vol\n",
    "        gamma = self.gamma_values[gamma_idx]\n",
    "        kappa = self.kappa_values[kappa_idx]\n",
    "        vol_corr = self.vol_corr_values[vol_corr_idx]\n",
    "\n",
    "        mid_price = self.prices[self.t]\n",
    "        sigma_est = 0.0\n",
    "        if self.t > 1:\n",
    "            start = max(0, self.t - int(self.T))\n",
    "            window_prices = self.prices[start:self.t+1]\n",
    "            if len(window_prices) > 1:\n",
    "                returns = np.diff(window_prices) / window_prices[:-1]\n",
    "                sigma_est = np.std(returns)\n",
    "        effective_sigma = sigma_est * vol_corr\n",
    "        if effective_sigma < 1e-8:\n",
    "            effective_sigma = 1e-8\n",
    "\n",
    "        reservation_price = mid_price - self.inventory * gamma * (effective_sigma ** 2) * self.T\n",
    "        delta = gamma * (effective_sigma ** 2) * self.T + (1.0 / gamma) * math.log(1 + gamma / kappa)\n",
    "        bid_price = reservation_price - delta / 2\n",
    "        ask_price = reservation_price + delta / 2\n",
    "\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "        if self.t < self.N - 1:\n",
    "            next_price = self.prices[self.t + 1]\n",
    "            if self.prices_high[self.t + 1] >= ask_price:\n",
    "                self.inventory -= 1.0\n",
    "                self.cash += ask_price * (1.0 - self.fee_rate)\n",
    "            if self.prices_low[self.t + 1] <= bid_price:\n",
    "                self.inventory += 1.0\n",
    "                self.cash -= bid_price * (1.0 + self.fee_rate)\n",
    "            current_value = self.cash + self.inventory * next_price\n",
    "            prev_value = self.cash + self.inventory * mid_price\n",
    "            pnl_change = current_value - prev_value\n",
    "            reward = pnl_change - 0.001 * abs(self.inventory)\n",
    "            self.t += 1\n",
    "        else:\n",
    "            done = True\n",
    "            current_value = self.cash + self.inventory * mid_price\n",
    "            reward = current_value - self.initial_cash\n",
    "\n",
    "        # Обновление истории относительных изменений цены\n",
    "        if self.t > 0:\n",
    "            rel_change = (self.prices[self.t] - self.prices[self.t - 1]) / self.prices[self.t - 1]\n",
    "        else:\n",
    "            rel_change = 0.0\n",
    "        self.price_change_history.pop(0)\n",
    "        self.price_change_history.append(rel_change)\n",
    "\n",
    "        if not done:\n",
    "            new_mid = self.prices[self.t]\n",
    "            pnl = self.cash + self.inventory * new_mid - self.initial_cash\n",
    "            obs = np.concatenate(([self.inventory], np.array(self.price_change_history), [pnl]))\n",
    "            obs = obs.astype(np.float32)\n",
    "        else:\n",
    "            obs = None\n",
    "        return obs, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нейросеть (2 головы: для данных и отдельно для PnL и Inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedNetwork(nn.Module):\n",
    "    def __init__(self, history_len=16, inv_pnl_dim=2, conv_out_channels=32, fc_inv_out=32,\n",
    "                 fc1_out=64, fc2_out=64):\n",
    "        super(CombinedNetwork, self).__init__()\n",
    "        # Сверточная ветвь для price history\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3)  # [batch, 16, L1]\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=conv_out_channels, kernel_size=3)  # [batch, 32, L2]\n",
    "        # После conv1: length = history_len - 3 + 1 = history_len -2, \n",
    "        # после conv2: length = (history_len -2) - 3 + 1 = history_len -4.\n",
    "        conv_feat_dim = conv_out_channels * (history_len - 4)\n",
    "        # Полносвязная ветвь для inventory и PnL (2 значения)\n",
    "        self.fc_inv = nn.Linear(inv_pnl_dim, fc_inv_out)\n",
    "        # Объединение: размерность = conv_feat_dim + fc_inv_out\n",
    "        combined_dim = conv_feat_dim + fc_inv_out\n",
    "        self.fc1 = nn.Linear(combined_dim, fc1_out)\n",
    "        self.fc2 = nn.Linear(fc1_out, fc2_out)\n",
    "    def forward(self, state):\n",
    "        # state shape: [batch, 18]: [inventory (1), price_history (16), pnl (1)]\n",
    "        inv = state[:, 0:1]             # [batch, 1]\n",
    "        price_history = state[:, 1:17]    # [batch, 16]\n",
    "        pnl = state[:, 17:18]             # [batch, 1]\n",
    "        inv_pnl = torch.cat([inv, pnl], dim=1)  # [batch, 2]\n",
    "        # Обработка price_history через сверточные слои\n",
    "        x_price = price_history.unsqueeze(1)    # [batch, 1, 16]\n",
    "        x_price = torch.relu(self.conv1(x_price)) # [batch, 16, 16-3+1 = 14]\n",
    "        x_price = torch.relu(self.conv2(x_price)) # [batch, conv_out_channels, 14-3+1 = 12]\n",
    "        x_price = x_price.view(x_price.size(0), -1)  # [batch, conv_out_channels * 12]\n",
    "        # Обработка inv_pnl через MLP\n",
    "        x_inv = torch.relu(self.fc_inv(inv_pnl))      # [batch, fc_inv_out]\n",
    "        # Объединение\n",
    "        x = torch.cat([x_price, x_inv], dim=1)         # [batch, conv_feat_dim + fc_inv_out]\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return x  # итоговое представление размера fc2_out (например, 64)\n",
    "\n",
    "# =============================================================================\n",
    "# Combined Q-Network для DQN агента\n",
    "# =============================================================================\n",
    "class CombinedQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, history_len=16):\n",
    "        super(CombinedQNetwork, self).__init__()\n",
    "        self.base = CombinedNetwork(history_len=history_len, inv_pnl_dim=2,\n",
    "                                     conv_out_channels=32, fc_inv_out=32,\n",
    "                                     fc1_out=64, fc2_out=64)\n",
    "        self.out = nn.Linear(64, action_dim)\n",
    "    def forward(self, state):\n",
    "        x = self.base(state)\n",
    "        q_values = self.out(x)\n",
    "        return q_values\n",
    "\n",
    "# =============================================================================\n",
    "# Combined Actor-Critic для A2C и PPO агентов\n",
    "# =============================================================================\n",
    "class CombinedActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, history_len=16):\n",
    "        super(CombinedActorCritic, self).__init__()\n",
    "        self.base = CombinedNetwork(history_len=history_len, inv_pnl_dim=2,\n",
    "                                     conv_out_channels=32, fc_inv_out=32,\n",
    "                                     fc1_out=64, fc2_out=64)\n",
    "        self.actor = nn.Linear(64, action_dim)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "    def forward(self, state):\n",
    "        x = self.base(state)\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "# =============================================================================\n",
    "# DQN агент с CombinedQNetwork\n",
    "# =============================================================================\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, \n",
    "                 buffer_capacity=10000, batch_size=64, target_update=1000, history_len=16):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = CombinedQNetwork(state_dim, action_dim, history_len=history_len).to(self.device)\n",
    "        self.target_network = CombinedQNetwork(state_dim, action_dim, history_len=history_len).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.steps_done = 0\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def push_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.FloatTensor(batch.state).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).unsqueeze(1).to(self.device)\n",
    "        non_final_mask = torch.tensor([s is not None for s in batch.next_state],\n",
    "                                      dtype=torch.bool, device=self.device)\n",
    "        non_final_next_states = torch.FloatTensor([s for s in batch.next_state if s is not None]).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch.done).unsqueeze(1).to(self.device)\n",
    "        q_values = self.q_network(state_batch).gather(1, action_batch)\n",
    "        next_q_values = torch.zeros(self.batch_size, 1).to(self.device)\n",
    "        if non_final_next_states.size(0) > 0:\n",
    "            next_q_values[non_final_mask] = self.target_network(non_final_next_states).max(1, keepdim=True)[0]\n",
    "        expected_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
    "        loss = nn.MSELoss()(q_values, expected_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.steps_done += 1\n",
    "        if self.steps_done % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "# =============================================================================\n",
    "# A2C агент с CombinedActorCritic\n",
    "# =============================================================================\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, \n",
    "                 value_coef=0.5, entropy_coef=0.01, history_len=16):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = CombinedActorCritic(state_dim, action_dim, history_len=history_len).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, value = self.model(state_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        action_tensor = torch.multinomial(probs, num_samples=1)\n",
    "        log_prob = torch.log(probs.gather(1, action_tensor) + 1e-10)\n",
    "        return action_tensor.item(), log_prob.squeeze(), value.squeeze(), logits.squeeze()\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        log_probs = torch.stack([t[2] for t in trajectories]).to(self.device)\n",
    "        values = torch.stack([t[3] for t in trajectories]).to(self.device)\n",
    "        rewards = [t[4] for t in trajectories]\n",
    "        dones = [t[5] for t in trajectories]\n",
    "        logits_list = torch.stack([t[6] for t in trajectories]).to(self.device)\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = returns - values\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        probs = torch.softmax(logits_list, dim=1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# =============================================================================\n",
    "# PPO агент с CombinedActorCritic\n",
    "# =============================================================================\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, clip_coef=0.2, \n",
    "                 value_coef=0.5, entropy_coef=0.01, ppo_epochs=4, history_len=16):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = CombinedActorCritic(state_dim, action_dim, history_len=history_len).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_coef = clip_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, value = self.model(state_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        action_tensor = torch.multinomial(probs, num_samples=1)\n",
    "        log_prob = torch.log(probs.gather(1, action_tensor) + 1e-10)\n",
    "        return action_tensor.item(), log_prob.squeeze(), value.squeeze()\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        states = torch.FloatTensor([t[0] for t in trajectories]).to(self.device)\n",
    "        actions = torch.LongTensor([t[1] for t in trajectories]).unsqueeze(1).to(self.device)\n",
    "        old_log_probs = torch.stack([t[2] for t in trajectories]).to(self.device)\n",
    "        old_values = torch.stack([t[3] for t in trajectories]).to(self.device)\n",
    "        rewards = [t[4] for t in trajectories]\n",
    "        dones = [t[5] for t in trajectories]\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        old_log_probs = old_log_probs.detach()\n",
    "        old_values = old_values.detach()\n",
    "        advantages = returns - old_values\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            logits, values = self.model(states)\n",
    "            values = values.squeeze(1)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            new_log_probs = torch.log(probs.gather(1, actions) + 1e-10).squeeze(1)\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            adv_detached = advantages.detach()\n",
    "            surr1 = ratio * adv_detached\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.clip_coef, 1.0 + self.clip_coef) * adv_detached\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (returns - values).pow(2).mean()\n",
    "            entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "            loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции для расчета метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_drawdown(pnl_history):\n",
    "    pnl_array = np.array(pnl_history)\n",
    "    running_max = np.maximum.accumulate(pnl_array)\n",
    "    drawdowns = pnl_array - running_max\n",
    "    return drawdowns.min()\n",
    "\n",
    "def compute_sharpe_ratio(pnl_history):\n",
    "    pnl_array = np.array(pnl_history)\n",
    "    returns = np.diff(pnl_array)\n",
    "    if returns.std() == 0:\n",
    "        return 0.0\n",
    "    sharpe = returns.mean() / returns.std() * np.sqrt(len(returns))\n",
    "    return sharpe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strike/work/penv/deep/lib/python3.13/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение DQN-агента...\n",
      "DQN Эпизод 1/20: суммарное вознаграждение = 699.70\n",
      "DQN Эпизод 2/20: суммарное вознаграждение = 2390.73\n",
      "DQN Эпизод 3/20: суммарное вознаграждение = 1681.28\n",
      "DQN Эпизод 4/20: суммарное вознаграждение = 872.65\n",
      "DQN Эпизод 5/20: суммарное вознаграждение = 825.03\n",
      "DQN Эпизод 6/20: суммарное вознаграждение = 8.10\n",
      "DQN Эпизод 7/20: суммарное вознаграждение = 3092.61\n",
      "DQN Эпизод 8/20: суммарное вознаграждение = 1879.47\n",
      "DQN Эпизод 9/20: суммарное вознаграждение = -292.12\n",
      "DQN Эпизод 10/20: суммарное вознаграждение = 388.23\n",
      "DQN Эпизод 11/20: суммарное вознаграждение = 2442.19\n",
      "DQN Эпизод 12/20: суммарное вознаграждение = 947.73\n",
      "DQN Эпизод 13/20: суммарное вознаграждение = 1096.97\n",
      "DQN Эпизод 14/20: суммарное вознаграждение = 1492.75\n",
      "DQN Эпизод 15/20: суммарное вознаграждение = 570.95\n",
      "DQN Эпизод 16/20: суммарное вознаграждение = -218.81\n",
      "DQN Эпизод 17/20: суммарное вознаграждение = 62.34\n",
      "DQN Эпизод 18/20: суммарное вознаграждение = -805.64\n",
      "DQN Эпизод 19/20: суммарное вознаграждение = 1663.98\n",
      "DQN Эпизод 20/20: суммарное вознаграждение = 228.81\n",
      "\n",
      "Обучение A2C-агента...\n",
      "A2C Эпизод 1/20: суммарное вознаграждение = 310.65\n",
      "A2C Эпизод 2/20: суммарное вознаграждение = 923.17\n",
      "A2C Эпизод 3/20: суммарное вознаграждение = 1410.13\n",
      "A2C Эпизод 4/20: суммарное вознаграждение = 335.89\n",
      "A2C Эпизод 5/20: суммарное вознаграждение = 933.94\n",
      "A2C Эпизод 6/20: суммарное вознаграждение = -372.34\n",
      "A2C Эпизод 7/20: суммарное вознаграждение = -155.21\n",
      "A2C Эпизод 8/20: суммарное вознаграждение = 1026.74\n",
      "A2C Эпизод 9/20: суммарное вознаграждение = 1064.27\n",
      "A2C Эпизод 10/20: суммарное вознаграждение = 853.48\n",
      "A2C Эпизод 11/20: суммарное вознаграждение = 534.86\n",
      "A2C Эпизод 12/20: суммарное вознаграждение = 125.57\n",
      "A2C Эпизод 13/20: суммарное вознаграждение = 155.20\n",
      "A2C Эпизод 14/20: суммарное вознаграждение = 428.58\n",
      "A2C Эпизод 15/20: суммарное вознаграждение = -429.88\n",
      "A2C Эпизод 16/20: суммарное вознаграждение = 1214.32\n",
      "A2C Эпизод 17/20: суммарное вознаграждение = 1133.66\n",
      "A2C Эпизод 18/20: суммарное вознаграждение = 710.51\n",
      "A2C Эпизод 19/20: суммарное вознаграждение = -1307.68\n",
      "A2C Эпизод 20/20: суммарное вознаграждение = 902.06\n",
      "\n",
      "Обучение PPO-агента...\n",
      "PPO Эпизод 1/20: суммарное вознаграждение = 2238.54\n",
      "PPO Эпизод 2/20: суммарное вознаграждение = 818.05\n",
      "PPO Эпизод 3/20: суммарное вознаграждение = 710.89\n",
      "PPO Эпизод 4/20: суммарное вознаграждение = 1608.23\n",
      "PPO Эпизод 5/20: суммарное вознаграждение = -725.22\n",
      "PPO Эпизод 6/20: суммарное вознаграждение = 400.60\n",
      "PPO Эпизод 7/20: суммарное вознаграждение = 792.92\n",
      "PPO Эпизод 8/20: суммарное вознаграждение = 754.58\n",
      "PPO Эпизод 9/20: суммарное вознаграждение = 435.85\n",
      "PPO Эпизод 10/20: суммарное вознаграждение = -373.53\n",
      "PPO Эпизод 11/20: суммарное вознаграждение = 1841.89\n",
      "PPO Эпизод 12/20: суммарное вознаграждение = 1015.09\n",
      "PPO Эпизод 13/20: суммарное вознаграждение = 111.33\n",
      "PPO Эпизод 14/20: суммарное вознаграждение = 1294.44\n",
      "PPO Эпизод 15/20: суммарное вознаграждение = 493.90\n",
      "PPO Эпизод 16/20: суммарное вознаграждение = 908.82\n",
      "PPO Эпизод 17/20: суммарное вознаграждение = 1229.67\n",
      "PPO Эпизод 18/20: суммарное вознаграждение = 1251.66\n",
      "PPO Эпизод 19/20: суммарное вознаграждение = 1571.58\n",
      "PPO Эпизод 20/20: суммарное вознаграждение = -1205.06\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Определяем дискретные наборы параметров для оптимизации\n",
    "# =============================================================================\n",
    "gamma_values = [0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "kappa_values = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
    "vol_corr_values = [0.5, 0.75, 1.0, 1.25, 1.5]  # поправка к sigma\n",
    "\n",
    "env = MarketMakingEnv(df=df, gamma_values=gamma_values, kappa_values=kappa_values, \n",
    "                      vol_corr_values=vol_corr_values, T=60.0, history_len=16)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "if isinstance(env.action_space, spaces.MultiDiscrete):\n",
    "    action_dim = int(np.prod(env.action_space.nvec))\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "# =============================================================================\n",
    "# Инициализируем агентов DQN, A2C и PPO\n",
    "# =============================================================================\n",
    "\n",
    "dqn_agent = DQNAgent(state_dim, action_dim, lr=1e-3, gamma=0.99, \n",
    "                     buffer_capacity=10000, batch_size=64, target_update=1000, history_len=16)\n",
    "a2c_agent = A2CAgent(state_dim, action_dim, lr=1e-3, gamma=0.99, \n",
    "                     value_coef=0.5, entropy_coef=0.01, history_len=16)\n",
    "ppo_agent = PPOAgent(state_dim, action_dim, lr=1e-3, gamma=0.99, clip_coef=0.2, \n",
    "                     value_coef=0.5, entropy_coef=0.01, ppo_epochs=4, history_len=16)'\n",
    "                     \n",
    "\n",
    "# =============================================================================\n",
    "# Параметры обучения\n",
    "# =============================================================================\n",
    "num_episodes = 20        # для DQN\n",
    "num_episodes_a2c = 20    # для A2C\n",
    "num_episodes_ppo = 20    # для PPO\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 300\n",
    "\n",
    "# =============================================================================\n",
    "# Обучение DQN-агента\n",
    "# =============================================================================\n",
    "print(\"Обучение DQN-агента...\")\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        epsilon = epsilon_final + (epsilon_start - epsilon_final) * math.exp(-env.t / epsilon_decay)\n",
    "        action = dqn_agent.select_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        dqn_agent.push_transition(state, action, reward, next_state, done)\n",
    "        dqn_agent.update()\n",
    "        state = next_state if next_state is not None else state\n",
    "        total_reward += reward\n",
    "    print(f\"DQN Эпизод {episode+1}/{num_episodes}: суммарное вознаграждение = {total_reward:.2f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Обучение A2C-агента\n",
    "# =============================================================================\n",
    "print(\"\\nОбучение A2C-агента...\")\n",
    "rollout_length = 10\n",
    "for episode in range(num_episodes_a2c):\n",
    "    state = env.reset()\n",
    "    trajectories = []\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        for _ in range(rollout_length):\n",
    "            action, log_prob, value, logits = a2c_agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectories.append((state, action, log_prob, value, reward, float(done), logits))\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        a2c_agent.update(trajectories)\n",
    "        trajectories = []\n",
    "    print(f\"A2C Эпизод {episode+1}/{num_episodes_a2c}: суммарное вознаграждение = {total_reward:.2f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Обучение PPO-агента\n",
    "# =============================================================================\n",
    "print(\"\\nОбучение PPO-агента...\")\n",
    "rollout_length_ppo = 20\n",
    "for episode in range(num_episodes_ppo):\n",
    "    state = env.reset()\n",
    "    trajectories = []\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        for _ in range(rollout_length_ppo):\n",
    "            action, log_prob, value = ppo_agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectories.append((state, action, log_prob, value, reward, float(done)))\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        ppo_agent.update(trajectories)\n",
    "        trajectories = []\n",
    "    print(f\"PPO Эпизод {episode+1}/{num_episodes_ppo}: суммарное вознаграждение = {total_reward:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strike/work/penv/deep/lib/python3.13/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты тестирования:\n",
      "DQN итоговый PnL: 119.86, Max Drawdown: -80.77, Sharpe Ratio: 0.83\n",
      "A2C итоговый PnL: 9.42, Max Drawdown: -29.29, Sharpe Ratio: 0.20\n",
      "PPO итоговый PnL: 43.67, Max Drawdown: -79.05, Sharpe Ratio: 0.53\n"
     ]
    }
   ],
   "source": [
    "def run_agent(env, agent, agent_type=\"dqn\"):\n",
    "    state = env.reset()\n",
    "    pnl_history = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        if agent_type == \"dqn\":\n",
    "            action = agent.select_action(state, epsilon=0.0)\n",
    "        elif agent_type == \"a2c\":\n",
    "            action, _, _, _ = agent.select_action(state)\n",
    "        elif agent_type == \"ppo\":\n",
    "            action, _, _ = agent.select_action(state)\n",
    "        else:\n",
    "            action = 0\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        if state is not None:\n",
    "            pnl_history.append(state[-1])  # последний элемент состояния – PnL\n",
    "        state = next_state if next_state is not None else state\n",
    "    return pnl_history\n",
    "\n",
    "env_test = MarketMakingEnv(df=df_test, gamma_values=gamma_values, kappa_values=kappa_values, \n",
    "                           vol_corr_values=vol_corr_values, T=60.0, history_len=16)\n",
    "dqn_pnl = run_agent(env_test, dqn_agent, agent_type=\"dqn\")\n",
    "env_test = MarketMakingEnv(df=df_test, gamma_values=gamma_values, kappa_values=kappa_values, \n",
    "                           vol_corr_values=vol_corr_values, T=60.0, history_len=16)\n",
    "a2c_pnl = run_agent(env_test, a2c_agent, agent_type=\"a2c\")\n",
    "env_test = MarketMakingEnv(df=df_test, gamma_values=gamma_values, kappa_values=kappa_values, \n",
    "                           vol_corr_values=vol_corr_values, T=60.0, history_len=16)\n",
    "ppo_pnl = run_agent(env_test, ppo_agent, agent_type=\"ppo\")\n",
    "\n",
    "dqn_max_dd = compute_max_drawdown(dqn_pnl)\n",
    "dqn_sharpe = compute_sharpe_ratio(dqn_pnl)\n",
    "a2c_max_dd = compute_max_drawdown(a2c_pnl)\n",
    "a2c_sharpe = compute_sharpe_ratio(a2c_pnl)\n",
    "ppo_max_dd = compute_max_drawdown(ppo_pnl)\n",
    "ppo_sharpe = compute_sharpe_ratio(ppo_pnl)\n",
    "\n",
    "print(\"\\nРезультаты тестирования:\")\n",
    "print(f\"DQN итоговый PnL: {dqn_pnl[-1]:.2f}, Max Drawdown: {dqn_max_dd:.2f}, Sharpe Ratio: {dqn_sharpe:.2f}\")\n",
    "print(f\"A2C итоговый PnL: {a2c_pnl[-1]:.2f}, Max Drawdown: {a2c_max_dd:.2f}, Sharpe Ratio: {a2c_sharpe:.2f}\")\n",
    "print(f\"PPO итоговый PnL: {ppo_pnl[-1]:.2f}, Max Drawdown: {ppo_max_dd:.2f}, Sharpe Ratio: {ppo_sharpe:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:  \n",
    "Для всех моделей получили положительный результат. Возможно подход со сверткой дает положительные результаты"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
