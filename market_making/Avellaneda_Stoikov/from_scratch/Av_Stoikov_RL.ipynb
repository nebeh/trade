{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Текущий подход по данным с дискретностью 1 мин\n",
    "(Минутные данные взяты для тестирования и проработки подхода, при модельная задержка также 1 минута) \n",
    "\n",
    "\n",
    "По модели Avellaneda-Stoikov делается расчет \n",
    "- реализованная цена\n",
    "$$r = S_t -q\\gamma\\sigma^2(T-t)$$\n",
    "- спред\n",
    "$$\\delta_a + \\delta_b = \\gamma\\sigma^2(T-t) + \\frac{1}{\\gamma}\\ln(1+\\frac{\\gamma}{k})$$\n",
    "\n",
    "### Параметр оптимизации (ищем RL методами), реализованный в текущем подходе\n",
    "- $\\gamma$ - уровень аверсии (ниприятия) к риску\n",
    "\n",
    "##### Какие еще параметры можем оптимизировать\n",
    "- T - время горизонта\n",
    "- k - параметр интенсивности сделок, который находится из соотношения $\\lambda(\\delta) = A\\exp(-k\\delta)$ (это если не находим этот параметр из реальных данных)\n",
    "- $\\Delta k$ поправку в параметр. Здесь мы принимаем, что $k = \\hat{k} + \\Delta k$, где $\\hat{k}$ оценка параметра из данных\n",
    "- $\\Delta\\sigma$ поправку в волатильность. Здесь мы принимаем, что $\\sigma = \\hat\\sigma + \\Delta\\sigma$, где $\\hat\\sigma$ оценка волатильности из данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дальнейшие улучшения \n",
    "1. Реализация на более высокочастотных таймфреймах\n",
    "2. Использовать книгу заказов в качестве observation для агента (скорее всего это сильно улучшит предсказательную способность модели)\n",
    "3. Реализовать RL схему, которая учит на прямую определять реализованную цену и спред (скорее всего результаты будут не устойчивые, но проверить можно, если давать агенту книгу заказов)\n",
    "4. Использовать более реалистичные модели рынка, например модель Мертона со скачками\n",
    "$$ dS_t = \\mu dt +\\sigma dW +JdN_t$$\n",
    "в которой резервированная цена считается по формуле\n",
    "$$r = S_t -q\\gamma\\sigma^2 (T-t) - \\lambda \\mathbb{E}[e^{-\\gamma J} - 1](T-t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from binance import Client\n",
    "from gym import spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time     open     high      low    close    volume  trades\n",
      "0 2025-03-17 08:27:00  1899.86  1900.92  1899.85  1900.15  155.0194   714.0\n",
      "1 2025-03-17 08:28:00  1900.15  1900.71  1899.88  1900.66   54.5553   611.0\n",
      "2 2025-03-17 08:29:00  1900.66  1900.67  1899.71  1900.53   89.4255   663.0\n",
      "3 2025-03-17 08:30:00  1900.54  1900.80  1898.90  1898.91   67.0853   994.0\n",
      "4 2025-03-17 08:31:00  1898.90  1899.66  1898.15  1899.04  895.3609  1285.0\n"
     ]
    }
   ],
   "source": [
    "# Инициализация клиента (без API ключей для публичных данных)\n",
    "client = Client()\n",
    "\n",
    "\n",
    "# Загрузка исторических данных\n",
    "def get_klines(symbol=\"ETHUSDT\", interval=\"1m\", start_date=\"6 day ago UTC\",end_date=\"2 day ago UTC\"):\n",
    "    try:\n",
    "        # Получение данных\n",
    "        klines = client.get_historical_klines(\n",
    "            symbol=symbol, interval=interval, start_str=start_date,end_str=end_date\n",
    "        )\n",
    "\n",
    "        # Преобразование в DataFrame\n",
    "        columns = [\n",
    "            \"time\",\n",
    "            \"open\",\n",
    "            \"high\",\n",
    "            \"low\",\n",
    "            \"close\",\n",
    "            \"volume\",\n",
    "            \"close_time\",\n",
    "            \"quote_volume\",\n",
    "            \"trades\",\n",
    "            \"taker_buy_base\",\n",
    "            \"taker_buy_quote\",\n",
    "            \"ignore\",\n",
    "        ]\n",
    "        df = pd.DataFrame(klines, columns=columns, dtype=float)\n",
    "\n",
    "        # Конвертация времени в читаемый формат\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"ms\")\n",
    "\n",
    "        return df[[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Получить данные за последние 24 часа (1-минутные, 1- секундные свечи)\n",
    "#df = get_klines(symbol=\"ETHUSDT\", interval=\"1s\")\n",
    "df = get_klines(symbol=\"ETHUSDT\", interval=\"1m\")\n",
    "# Вывод первых 5 строк\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df)\n",
    "ratio = 0.8\n",
    "df_test = df[int(N * ratio) :]\n",
    "df = df[: int(N * ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В этом подходе мы настраиваем параметр гамма с помощью RL для максимизации PnL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Среда\n",
    "- **Действие**: выбор $\\gamma$ из квантовонного набора\n",
    "- **Наблюдение**: inventory, относительное изменение цены (return), PnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketMakingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Окружение маркет-мейкинга по Avellaneda-Stoikov с учётом задержки (latency = 1 сек #TODO).\n",
    "    Состояние: [inventory, относительное изменение цены, PnL]\n",
    "    Действие: выбор индекса для γ из заданного списка (например, [0.01, 0.1, 0.5, 1.0])\n",
    "\n",
    "    #TODO\n",
    "    - kappa - параметра интенсивности, определяется из книги заказов\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, gamma_values, T=60.0, kappa=1e-3):\n",
    "        super(MarketMakingEnv, self).__init__()\n",
    "        self.prices = df.close.values\n",
    "        self.prices_high = df.high.values\n",
    "        self.prices_low = df.low.values\n",
    "        self.N = len(df)\n",
    "        self.gamma_values = gamma_values\n",
    "        self.T = T  # горизонт торгов (сек)\n",
    "        self.kappa = kappa  # параметр ликвидности\n",
    "        # Определяем пространства:\n",
    "        # Действие – дискретный выбор из len(gamma_values)\n",
    "        self.action_space = spaces.Discrete(len(gamma_values))\n",
    "        # Состояние: [inventory, relative price change, PnL]\n",
    "        high = np.array([100.0, 1.0, 1e9], dtype=np.float32)\n",
    "        low = np.array([-100.0, -1.0, -1e9], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "        self.initial_cash = 10000.0\n",
    "        self.fee_rate = 0.0  # можно задать комиссию\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.inventory = 0.0\n",
    "        self.cash = self.initial_cash\n",
    "        self.prev_price = self.prices[self.t]\n",
    "        # Начальное состояние: инвентарь 0, изменения цены = 0, PnL = 0\n",
    "        obs = np.array([self.inventory, 0.0, 0.0], dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        gamma = self.gamma_values[action]\n",
    "        mid_price = self.prices[self.t]\n",
    "        # Оценка волатильности на окне последних 60 секунд\n",
    "        sigma = 0.0\n",
    "        if self.t > 1:\n",
    "            start = max(0, self.t - 60)\n",
    "            window_prices = self.prices[start : self.t + 1]\n",
    "            if len(window_prices) > 1:\n",
    "                returns = np.diff(window_prices) / np.array(window_prices[:-1])\n",
    "                sigma = np.std(returns)\n",
    "        # Вычисляем цену резервирования и ширину спреда\n",
    "        reservation_price = mid_price - self.inventory * gamma * (sigma**2) * self.T\n",
    "        delta = gamma * (sigma**2) * self.T + (1.0 / gamma) * math.log(\n",
    "            1 + gamma / self.kappa\n",
    "        )\n",
    "\n",
    "        bid_price = reservation_price - delta / 2\n",
    "        ask_price = reservation_price + delta / 2\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "        # Если ещё есть данные для исполнения (latency = 1 мин, т.е. смотрим на цену следующей минуты)\n",
    "        if self.t < self.N - 2:\n",
    "            next_price = self.prices[self.t + 2]\n",
    "            # Если цена выше ask – исполняется заявка на продажу (продажа 1 единицы)\n",
    "            if self.prices_high[self.t + 2] >= ask_price:\n",
    "                self.inventory -= 1.0\n",
    "                self.cash += ask_price * (1.0 - self.fee_rate)\n",
    "            # Если цена ниже bid – исполняется заявка на покупку (покупка 1 единицы)\n",
    "            if self.prices_low[self.t + 2] <= bid_price:\n",
    "                self.inventory += 1.0\n",
    "                self.cash -= bid_price * (1.0 + self.fee_rate)\n",
    "            current_value = self.cash + self.inventory * next_price\n",
    "            prev_value = self.cash + self.inventory * mid_price\n",
    "            pnl_change = current_value - prev_value\n",
    "            # Небольшой штраф за большую позицию\n",
    "            reward = pnl_change - 0.001 * abs(self.inventory)\n",
    "            self.t += 1\n",
    "        else:\n",
    "            done = True\n",
    "            current_value = self.cash + self.inventory * mid_price\n",
    "            reward = current_value - self.initial_cash\n",
    "        if not done:\n",
    "            new_mid = self.prices[self.t]\n",
    "            price_change = (new_mid - mid_price) / mid_price\n",
    "            pnl = self.cash + self.inventory * new_mid - self.initial_cash\n",
    "            obs = np.array([self.inventory, price_change, pnl], dtype=np.float32)\n",
    "        else:\n",
    "            obs = None\n",
    "        return obs, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Агенты\n",
    "Используем DQN и A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# DQN-агент\n",
    "\n",
    "# Определяем простую Q-сеть\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "Transition = namedtuple(\n",
    "    \"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        buffer_capacity=10000,\n",
    "        batch_size=64,\n",
    "        target_update=1000,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.steps_done = 0\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def push_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.FloatTensor(batch.state).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).unsqueeze(1).to(self.device)\n",
    "        non_final_mask = torch.tensor(\n",
    "            [s is not None for s in batch.next_state],\n",
    "            dtype=torch.bool,\n",
    "            device=self.device,\n",
    "        )\n",
    "        non_final_next_states = torch.FloatTensor(\n",
    "            [s for s in batch.next_state if s is not None]\n",
    "        ).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch.done).unsqueeze(1).to(self.device)\n",
    "        # Q(s,a)\n",
    "        q_values = self.q_network(state_batch).gather(1, action_batch)\n",
    "        # Q_target\n",
    "        next_q_values = torch.zeros(self.batch_size, 1).to(self.device)\n",
    "        if non_final_next_states.size(0) > 0:\n",
    "            next_q_values[non_final_mask] = self.target_network(\n",
    "                non_final_next_states\n",
    "            ).max(1, keepdim=True)[0]\n",
    "        expected_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
    "        loss = nn.MSELoss()(q_values, expected_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.steps_done += 1\n",
    "        if self.steps_done % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# A2C-агент\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.actor = nn.Linear(64, action_dim)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ActorCritic(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, value = self.model(state_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        action = torch.multinomial(probs, num_samples=1)\n",
    "        log_prob = torch.log(probs.gather(1, action))\n",
    "        return action.item(), log_prob.squeeze(), value.squeeze(), logits.squeeze()\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        # trajectories: список кортежей (state, action, log_prob, value, reward, done, logits)\n",
    "        # TODO\n",
    "        # states = torch.FloatTensor([t[0] for t in trajectories]).to(self.device)\n",
    "        # actions = torch.LongTensor([t[1] for t in trajectories]).to(self.device)\n",
    "        log_probs = torch.stack([t[2] for t in trajectories]).to(self.device)\n",
    "        values = torch.stack([t[3] for t in trajectories]).to(self.device)\n",
    "        rewards = [t[4] for t in trajectories]\n",
    "        dones = [t[5] for t in trajectories]\n",
    "        logits_list = torch.stack([t[6] for t in trajectories]).to(self.device)\n",
    "\n",
    "        # Вычисляем кумулятивную сумму вознаграждений (returns) обратным проходом\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = returns - values\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        probs = torch.softmax(logits_list, dim=1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение DQN-агента...\n",
      "Эпизод 1/20: суммарное вознаграждение = -129.64\n",
      "Эпизод 2/20: суммарное вознаграждение = 1183.26\n",
      "Эпизод 3/20: суммарное вознаграждение = -368.49\n",
      "Эпизод 4/20: суммарное вознаграждение = -1550.74\n",
      "Эпизод 5/20: суммарное вознаграждение = -164.36\n",
      "Эпизод 6/20: суммарное вознаграждение = -168.41\n",
      "Эпизод 7/20: суммарное вознаграждение = -1215.00\n",
      "Эпизод 8/20: суммарное вознаграждение = -3100.88\n",
      "Эпизод 9/20: суммарное вознаграждение = 3275.89\n",
      "Эпизод 10/20: суммарное вознаграждение = 545.50\n",
      "Эпизод 11/20: суммарное вознаграждение = 286.96\n",
      "Эпизод 12/20: суммарное вознаграждение = 227.02\n",
      "Эпизод 13/20: суммарное вознаграждение = -1079.82\n",
      "Эпизод 14/20: суммарное вознаграждение = -1773.30\n",
      "Эпизод 15/20: суммарное вознаграждение = -3996.40\n",
      "Эпизод 16/20: суммарное вознаграждение = 1420.88\n",
      "Эпизод 17/20: суммарное вознаграждение = -416.03\n",
      "Эпизод 18/20: суммарное вознаграждение = 339.07\n",
      "Эпизод 19/20: суммарное вознаграждение = 1572.82\n",
      "Эпизод 20/20: суммарное вознаграждение = 2904.63\n",
      "\n",
      "Обучение A2C-агента...\n",
      "Эпизод 1/20: суммарное вознаграждение = -1708.79\n",
      "Эпизод 2/20: суммарное вознаграждение = -729.24\n",
      "Эпизод 3/20: суммарное вознаграждение = -2006.39\n",
      "Эпизод 4/20: суммарное вознаграждение = -1440.05\n",
      "Эпизод 5/20: суммарное вознаграждение = 723.42\n",
      "Эпизод 6/20: суммарное вознаграждение = -535.53\n",
      "Эпизод 7/20: суммарное вознаграждение = -711.48\n",
      "Эпизод 8/20: суммарное вознаграждение = 1290.31\n",
      "Эпизод 9/20: суммарное вознаграждение = -4110.86\n",
      "Эпизод 10/20: суммарное вознаграждение = -1965.45\n",
      "Эпизод 11/20: суммарное вознаграждение = -69.65\n",
      "Эпизод 12/20: суммарное вознаграждение = 1508.66\n",
      "Эпизод 13/20: суммарное вознаграждение = -4219.84\n",
      "Эпизод 14/20: суммарное вознаграждение = -918.30\n",
      "Эпизод 15/20: суммарное вознаграждение = 1979.35\n",
      "Эпизод 16/20: суммарное вознаграждение = 1072.64\n",
      "Эпизод 17/20: суммарное вознаграждение = -1367.34\n",
      "Эпизод 18/20: суммарное вознаграждение = 3872.24\n",
      "Эпизод 19/20: суммарное вознаграждение = -1958.67\n",
      "Эпизод 20/20: суммарное вознаграждение = -1390.05\n"
     ]
    }
   ],
   "source": [
    "# Задаём параметры окружения и агентов\n",
    "gamma_values = [0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "env = MarketMakingEnv(df=df, gamma_values=gamma_values, T=60.0, kappa=1e-3)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Параметры для обучения DQN\n",
    "\n",
    "dqn_agent = DQNAgent(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    buffer_capacity=10000,\n",
    "    batch_size=64,\n",
    "    target_update=1000,\n",
    ")\n",
    "\n",
    "\n",
    "num_episodes = 20\n",
    "num_episodes_a2c = 20\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 300\n",
    "\n",
    "print(\"Обучение DQN-агента...\")\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # ε-жадная политика\n",
    "        epsilon = epsilon_final + (epsilon_start - epsilon_final) * math.exp(\n",
    "            -env.t / epsilon_decay\n",
    "        )\n",
    "        action = dqn_agent.select_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        dqn_agent.push_transition(state, action, reward, next_state, done)\n",
    "        dqn_agent.update()\n",
    "        state = next_state if next_state is not None else state\n",
    "        total_reward += reward\n",
    "    print(\n",
    "        f\"Эпизод {episode+1}/{num_episodes}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "# Обучение A2C-агента\n",
    "\n",
    "a2c_agent = A2CAgent(\n",
    "    state_dim, action_dim, lr=1e-3, gamma=0.99, value_coef=0.5, entropy_coef=0.01\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nОбучение A2C-агента...\")\n",
    "\n",
    "rollout_length = 10  # собираем траектории по 10 шагов\n",
    "for episode in range(num_episodes_a2c):\n",
    "    state = env.reset()\n",
    "    trajectories = []\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Собираем траекторию rollout_length\n",
    "        for _ in range(rollout_length):\n",
    "            action, log_prob, value, logits = a2c_agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectories.append(\n",
    "                (state, action, log_prob, value, reward, float(done), logits)\n",
    "            )\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        a2c_agent.update(trajectories)\n",
    "        trajectories = []\n",
    "    print(\n",
    "        f\"Эпизод {episode+1}/{num_episodes_a2c}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты тестирования:\n",
      "DQN итоговый PnL: 0.00\n",
      "A2C итоговый PnL: 142.22\n"
     ]
    }
   ],
   "source": [
    "def run_agent(env, agent, agent_type=\"dqn\"):\n",
    "    state = env.reset()\n",
    "    pnl_history = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        if agent_type == \"dqn\":\n",
    "            action = agent.select_action(state, epsilon=0.0)\n",
    "        elif agent_type == \"a2c\":\n",
    "            action, _, _, _ = agent.select_action(state)\n",
    "        else:\n",
    "            action = 0  # базовая стратегия\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        # Третий элемент состояния – PnL\n",
    "        if state is not None:\n",
    "            pnl = state[2]\n",
    "            pnl_history.append(pnl)\n",
    "        state = next_state if next_state is not None else state\n",
    "    return pnl_history\n",
    "\n",
    "\n",
    "# Прогон теста для обоих агентов\n",
    "env_test = MarketMakingEnv(df=df_test, gamma_values=gamma_values, T=60.0, kappa=1e-3)\n",
    "dqn_pnl = run_agent(env_test, dqn_agent, agent_type=\"dqn\")\n",
    "env_test = MarketMakingEnv(df=df_test, gamma_values=gamma_values, T=60.0, kappa=1e-3)\n",
    "a2c_pnl = run_agent(env_test, a2c_agent, agent_type=\"a2c\")\n",
    "\n",
    "print(\"\\nРезультаты тестирования:\")\n",
    "print(f\"DQN итоговый PnL: {dqn_pnl[-1]:.2f}\")\n",
    "print(f\"A2C итоговый PnL: {a2c_pnl[-1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:  \n",
    "При минутном пересчете лучшие показатели у A2C агента. Смоделированная задержка (latency) в 1 мин, но фактическая задержка составляет от нескольких сот миллисекунд до нескольких секунд"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
