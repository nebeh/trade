{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from binance import Client\n",
    "from gym import spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные пары ETHUSDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time     open     high      low    close      volume   trades\n",
      "0 2025-03-17 12:30:00  1909.47  1939.03  1908.69  1921.37  10708.8771  20466.0\n",
      "1 2025-03-17 12:31:00  1921.36  1922.00  1917.41  1917.98   1294.7324   5829.0\n",
      "2 2025-03-17 12:32:00  1917.98  1923.49  1917.49  1922.61    873.2570   4396.0\n",
      "3 2025-03-17 12:33:00  1922.58  1923.91  1918.80  1920.24    767.8835   3206.0\n",
      "4 2025-03-17 12:34:00  1920.25  1921.65  1919.97  1920.38    359.2940   1903.0\n"
     ]
    }
   ],
   "source": [
    "# Инициализация клиента Binance (публичный доступ)\n",
    "client = Client()\n",
    "\n",
    "\n",
    "# Загрузка исторических данных (свечи)\n",
    "def get_klines(\n",
    "    symbol=\"ETHUSDT\",\n",
    "    interval=\"1m\",\n",
    "    start_date=\"6 day ago UTC\",\n",
    "    end_date=\"2 day ago UTC\",\n",
    "):\n",
    "    try:\n",
    "        klines = client.get_historical_klines(\n",
    "            symbol=symbol, interval=interval, start_str=start_date, end_str=end_date\n",
    "        )\n",
    "        columns = [\n",
    "            \"time\",\n",
    "            \"open\",\n",
    "            \"high\",\n",
    "            \"low\",\n",
    "            \"close\",\n",
    "            \"volume\",\n",
    "            \"close_time\",\n",
    "            \"quote_volume\",\n",
    "            \"trades\",\n",
    "            \"taker_buy_base\",\n",
    "            \"taker_buy_quote\",\n",
    "            \"ignore\",\n",
    "        ]\n",
    "        df = pd.DataFrame(klines, columns=columns, dtype=float)\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"ms\")\n",
    "        return df[[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при получении данных: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "df = get_klines(symbol=\"ETHUSDT\", interval=\"1m\")\n",
    "print(df.head())\n",
    "\n",
    "# Разбиваем данные на обучающую и тестовую выборки\n",
    "if df is not None:\n",
    "    N = len(df)\n",
    "    ratio = 0.8\n",
    "    df_test = df[int(N * ratio) :].reset_index(drop=True)\n",
    "    df = df[: int(N * ratio)].reset_index(drop=True)\n",
    "else:\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]\n",
    "    )\n",
    "    df_test = pd.DataFrame(\n",
    "        columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Среда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketMakingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Окружение маркет-мейкинга по Avellaneda-Stoikov с использованием свечных данных.\n",
    "    Состояние: [inventory, относительное изменение цены, PnL].\n",
    "    Действие: мультидискретный выбор (γ, κ, Δσ), где:\n",
    "      - γ: коэффициент аверсии к риску,\n",
    "      - κ: интенсивность ликвидности,\n",
    "      - Δσ: поправка к оценённой волатильности.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, gamma_values, kappa_values, vol_corr_values, T=60.0):\n",
    "        super(MarketMakingEnv, self).__init__()\n",
    "        self.prices = df.close.values\n",
    "        self.prices_high = df.high.values\n",
    "        self.prices_low = df.low.values\n",
    "        self.N = len(df)\n",
    "        self.gamma_values = gamma_values  # список допустимых значений γ\n",
    "        self.kappa_values = kappa_values  # список допустимых значений κ\n",
    "        self.vol_corr_values = vol_corr_values  # список поправок к σ (Δσ)\n",
    "        self.T = T  # торговый горизонт (секунд) для расчёта оптимальных котировок\n",
    "        # Пространство действий: мультидискретное – [#γ, #κ, #Δσ]\n",
    "        self.action_space = spaces.MultiDiscrete(\n",
    "            [len(gamma_values), len(kappa_values), len(vol_corr_values)]\n",
    "        )\n",
    "        # Пространство состояний: [inventory, relative price change, PnL]\n",
    "        high = np.array([100.0, 1.0, 1e9], dtype=np.float32)\n",
    "        low = np.array([-100.0, -1.0, -1e9], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "        self.initial_cash = 10000.0\n",
    "        self.fee_rate = 0.0  # комиссия\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.inventory = 0.0\n",
    "        self.cash = self.initial_cash\n",
    "        return np.array([self.inventory, 0.0, 0.0], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Раскодирование мультидискретного действия:\n",
    "        n_gamma = len(self.gamma_values)\n",
    "        n_kappa = len(self.kappa_values)\n",
    "        n_vol = len(self.vol_corr_values)\n",
    "        if isinstance(action, (np.ndarray, list, tuple)):\n",
    "            gamma_idx = int(action[0])\n",
    "            kappa_idx = int(action[1])\n",
    "            vol_corr_idx = int(action[2])\n",
    "        else:\n",
    "            action = int(action)\n",
    "            gamma_idx = action // (n_gamma * n_vol)\n",
    "            rem = action % (n_kappa * n_vol)\n",
    "            kappa_idx = rem // n_vol\n",
    "            vol_corr_idx = rem % n_vol\n",
    "\n",
    "        gamma = self.gamma_values[gamma_idx]\n",
    "        kappa = self.kappa_values[kappa_idx]\n",
    "        vol_corr = self.vol_corr_values[vol_corr_idx]\n",
    "\n",
    "        mid_price = self.prices[self.t]\n",
    "        # Оценка волатильности по окну длиной T (в секундах)\n",
    "        sigma_est = 0.0\n",
    "        if self.t > 1:\n",
    "            start = max(0, self.t - int(self.T))\n",
    "            window_prices = self.prices[start : self.t + 1]\n",
    "            if len(window_prices) > 1:\n",
    "                returns = np.diff(window_prices) / window_prices[:-1]\n",
    "                sigma_est = np.std(returns)\n",
    "        # Применяем поправку: эффективная волатильность = оценка + поправка\n",
    "        effective_sigma = sigma_est * vol_corr\n",
    "        if effective_sigma < 1e-8:\n",
    "            effective_sigma = 1e-8\n",
    "\n",
    "        # Расчёт цены резервирования:\n",
    "        reservation_price = (\n",
    "            mid_price - self.inventory * gamma * (effective_sigma**2) * self.T\n",
    "        )\n",
    "        # Оптимальный спред (полуширина):\n",
    "        # Формула: delta = γ * σ^2 * T + (1/γ) * ln(1 + γ/κ)\n",
    "        delta = gamma * (effective_sigma**2) * self.T + (1.0 / gamma) * math.log(\n",
    "            1 + gamma / kappa\n",
    "        )\n",
    "        bid_price = reservation_price - delta / 2\n",
    "        ask_price = reservation_price + delta / 2\n",
    "\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "        if self.t < self.N - 1:\n",
    "            next_price = self.prices[self.t + 1]\n",
    "            # Проверяем исполнение заявок по свечным данным:\n",
    "            if self.prices_high[self.t + 1] >= ask_price:\n",
    "                # Продажа: уменьшаем инвентарь, увеличиваем cash\n",
    "                self.inventory -= 1.0\n",
    "                self.cash += ask_price * (1.0 - self.fee_rate)\n",
    "            if self.prices_low[self.t + 1] <= bid_price:\n",
    "                # Покупка: увеличиваем инвентарь, уменьшаем cash\n",
    "                self.inventory += 1.0\n",
    "                self.cash -= bid_price * (1.0 + self.fee_rate)\n",
    "            current_value = self.cash + self.inventory * next_price\n",
    "            prev_value = self.cash + self.inventory * mid_price\n",
    "            pnl_change = current_value - prev_value\n",
    "            reward = pnl_change - 0.001 * abs(self.inventory)\n",
    "            self.t += 1\n",
    "        else:\n",
    "            done = True\n",
    "            current_value = self.cash + self.inventory * mid_price\n",
    "            reward = current_value - self.initial_cash\n",
    "\n",
    "        if not done:\n",
    "            new_mid = self.prices[self.t]\n",
    "            price_change = (new_mid - mid_price) / mid_price\n",
    "            pnl = self.cash + self.inventory * new_mid - self.initial_cash\n",
    "            obs = np.array([self.inventory, price_change, pnl], dtype=np.float32)\n",
    "        else:\n",
    "            obs = None\n",
    "        return obs, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Агенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# DQN-агент\n",
    "###############################################################################\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "Transition = namedtuple(\n",
    "    \"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        buffer_capacity=10000,\n",
    "        batch_size=64,\n",
    "        target_update=1000,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.steps_done = 0\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def push_transition(self, state, action, reward, next_state, done):\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.FloatTensor(batch.state).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).unsqueeze(1).to(self.device)\n",
    "        non_final_mask = torch.tensor(\n",
    "            [s is not None for s in batch.next_state],\n",
    "            dtype=torch.bool,\n",
    "            device=self.device,\n",
    "        )\n",
    "        non_final_next_states = torch.FloatTensor(\n",
    "            [s for s in batch.next_state if s is not None]\n",
    "        ).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch.done).unsqueeze(1).to(self.device)\n",
    "        q_values = self.q_network(state_batch).gather(1, action_batch)\n",
    "        next_q_values = torch.zeros(self.batch_size, 1).to(self.device)\n",
    "        if non_final_next_states.size(0) > 0:\n",
    "            next_q_values[non_final_mask] = self.target_network(\n",
    "                non_final_next_states\n",
    "            ).max(1, keepdim=True)[0]\n",
    "        expected_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
    "        loss = nn.MSELoss()(q_values, expected_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.steps_done += 1\n",
    "        if self.steps_done % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# A2C-агент\n",
    "###############################################################################\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.actor = nn.Linear(64, action_dim)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ActorCritic(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, value = self.model(state_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        action_tensor = torch.multinomial(probs, num_samples=1)\n",
    "        log_prob = torch.log(probs.gather(1, action_tensor) + 1e-10)\n",
    "        return (\n",
    "            action_tensor.item(),\n",
    "            log_prob.squeeze(),\n",
    "            value.squeeze(),\n",
    "            logits.squeeze(),\n",
    "        )\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        log_probs = torch.stack([t[2] for t in trajectories]).to(self.device)\n",
    "        values = torch.stack([t[3] for t in trajectories]).to(self.device)\n",
    "        rewards = [t[4] for t in trajectories]\n",
    "        dones = [t[5] for t in trajectories]\n",
    "        logits_list = torch.stack([t[6] for t in trajectories]).to(self.device)\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = returns - values\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        probs = torch.softmax(logits_list, dim=1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# PPO-агент\n",
    "###############################################################################\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        clip_coef=0.2,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "        ppo_epochs=4,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ActorCritic(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_coef = clip_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, value = self.model(state_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        action_tensor = torch.multinomial(probs, num_samples=1)\n",
    "        log_prob = torch.log(probs.gather(1, action_tensor) + 1e-10)\n",
    "        return action_tensor.item(), log_prob.squeeze(), value.squeeze()\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        # Преобразуем траектории в тензоры\n",
    "        states = torch.FloatTensor([t[0] for t in trajectories]).to(self.device)\n",
    "        actions = (\n",
    "            torch.LongTensor([t[1] for t in trajectories]).unsqueeze(1).to(self.device)\n",
    "        )\n",
    "        old_log_probs = torch.stack([t[2] for t in trajectories]).to(self.device)\n",
    "        old_values = torch.stack([t[3] for t in trajectories]).to(self.device)\n",
    "        rewards = [t[4] for t in trajectories]\n",
    "        dones = [t[5] for t in trajectories]\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        old_log_probs = old_log_probs.detach()\n",
    "        old_values = old_values.detach()\n",
    "        advantages = returns - old_values\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            logits, values = self.model(states)\n",
    "            values = values.squeeze(1)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            new_log_probs = torch.log(probs.gather(1, actions) + 1e-10).squeeze(1)\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            adv_detached = advantages.detach()\n",
    "            surr1 = ratio * adv_detached\n",
    "            surr2 = (\n",
    "                torch.clamp(ratio, 1.0 - self.clip_coef, 1.0 + self.clip_coef)\n",
    "                * adv_detached\n",
    "            )\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (returns - values).pow(2).mean()\n",
    "            entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "            loss = (\n",
    "                actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "            )\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение DQN-агента...\n",
      "DQN Эпизод 1/30: суммарное вознаграждение = 1410.09\n",
      "DQN Эпизод 2/30: суммарное вознаграждение = 642.54\n",
      "DQN Эпизод 3/30: суммарное вознаграждение = -401.15\n",
      "DQN Эпизод 4/30: суммарное вознаграждение = 712.18\n",
      "DQN Эпизод 5/30: суммарное вознаграждение = 826.60\n",
      "DQN Эпизод 6/30: суммарное вознаграждение = 565.99\n",
      "DQN Эпизод 7/30: суммарное вознаграждение = 5131.27\n",
      "DQN Эпизод 8/30: суммарное вознаграждение = 347.03\n",
      "DQN Эпизод 9/30: суммарное вознаграждение = 1010.62\n",
      "DQN Эпизод 10/30: суммарное вознаграждение = -1405.53\n",
      "DQN Эпизод 11/30: суммарное вознаграждение = -901.37\n",
      "DQN Эпизод 12/30: суммарное вознаграждение = -1001.68\n",
      "DQN Эпизод 13/30: суммарное вознаграждение = 1433.37\n",
      "DQN Эпизод 14/30: суммарное вознаграждение = 1818.98\n",
      "DQN Эпизод 15/30: суммарное вознаграждение = 2058.62\n",
      "DQN Эпизод 16/30: суммарное вознаграждение = 617.54\n",
      "DQN Эпизод 17/30: суммарное вознаграждение = 3792.89\n",
      "DQN Эпизод 18/30: суммарное вознаграждение = -2292.75\n",
      "DQN Эпизод 19/30: суммарное вознаграждение = -271.64\n",
      "DQN Эпизод 20/30: суммарное вознаграждение = -451.78\n",
      "DQN Эпизод 21/30: суммарное вознаграждение = 2134.82\n",
      "DQN Эпизод 22/30: суммарное вознаграждение = -1588.66\n",
      "DQN Эпизод 23/30: суммарное вознаграждение = 1668.64\n",
      "DQN Эпизод 24/30: суммарное вознаграждение = -1867.26\n",
      "DQN Эпизод 25/30: суммарное вознаграждение = -1468.37\n",
      "DQN Эпизод 26/30: суммарное вознаграждение = -2378.61\n",
      "DQN Эпизод 27/30: суммарное вознаграждение = 1677.83\n",
      "DQN Эпизод 28/30: суммарное вознаграждение = 1719.87\n",
      "DQN Эпизод 29/30: суммарное вознаграждение = 102.97\n",
      "DQN Эпизод 30/30: суммарное вознаграждение = -51.67\n",
      "\n",
      "Обучение A2C-агента...\n",
      "A2C Эпизод 1/30: суммарное вознаграждение = -1222.56\n",
      "A2C Эпизод 2/30: суммарное вознаграждение = -19.76\n",
      "A2C Эпизод 3/30: суммарное вознаграждение = -128.31\n",
      "A2C Эпизод 4/30: суммарное вознаграждение = 1248.28\n",
      "A2C Эпизод 5/30: суммарное вознаграждение = 424.63\n",
      "A2C Эпизод 6/30: суммарное вознаграждение = 509.46\n",
      "A2C Эпизод 7/30: суммарное вознаграждение = 953.46\n",
      "A2C Эпизод 8/30: суммарное вознаграждение = 2077.67\n",
      "A2C Эпизод 9/30: суммарное вознаграждение = 918.33\n",
      "A2C Эпизод 10/30: суммарное вознаграждение = 1834.22\n",
      "A2C Эпизод 11/30: суммарное вознаграждение = 2033.49\n",
      "A2C Эпизод 12/30: суммарное вознаграждение = 1818.76\n",
      "A2C Эпизод 13/30: суммарное вознаграждение = 1697.78\n",
      "A2C Эпизод 14/30: суммарное вознаграждение = 1974.55\n",
      "A2C Эпизод 15/30: суммарное вознаграждение = 22.04\n",
      "A2C Эпизод 16/30: суммарное вознаграждение = 615.70\n",
      "A2C Эпизод 17/30: суммарное вознаграждение = 1704.92\n",
      "A2C Эпизод 18/30: суммарное вознаграждение = 109.04\n",
      "A2C Эпизод 19/30: суммарное вознаграждение = 1683.68\n",
      "A2C Эпизод 20/30: суммарное вознаграждение = 2174.49\n",
      "A2C Эпизод 21/30: суммарное вознаграждение = 255.32\n",
      "A2C Эпизод 22/30: суммарное вознаграждение = 1691.31\n",
      "A2C Эпизод 23/30: суммарное вознаграждение = -507.18\n",
      "A2C Эпизод 24/30: суммарное вознаграждение = 2859.14\n",
      "A2C Эпизод 25/30: суммарное вознаграждение = 2316.49\n",
      "A2C Эпизод 26/30: суммарное вознаграждение = 2442.79\n",
      "A2C Эпизод 27/30: суммарное вознаграждение = 3179.54\n",
      "A2C Эпизод 28/30: суммарное вознаграждение = 1197.23\n",
      "A2C Эпизод 29/30: суммарное вознаграждение = 1721.19\n",
      "A2C Эпизод 30/30: суммарное вознаграждение = 1687.84\n",
      "\n",
      "Обучение PPO-агента...\n",
      "PPO Эпизод 1/30: суммарное вознаграждение = -1791.36\n",
      "PPO Эпизод 2/30: суммарное вознаграждение = 2540.51\n",
      "PPO Эпизод 3/30: суммарное вознаграждение = -320.55\n",
      "PPO Эпизод 4/30: суммарное вознаграждение = 774.22\n",
      "PPO Эпизод 5/30: суммарное вознаграждение = 1087.66\n",
      "PPO Эпизод 6/30: суммарное вознаграждение = 2816.22\n",
      "PPO Эпизод 7/30: суммарное вознаграждение = 625.50\n",
      "PPO Эпизод 8/30: суммарное вознаграждение = 441.45\n",
      "PPO Эпизод 9/30: суммарное вознаграждение = 469.08\n",
      "PPO Эпизод 10/30: суммарное вознаграждение = 916.29\n",
      "PPO Эпизод 11/30: суммарное вознаграждение = 891.93\n",
      "PPO Эпизод 12/30: суммарное вознаграждение = 2585.84\n",
      "PPO Эпизод 13/30: суммарное вознаграждение = 1643.82\n",
      "PPO Эпизод 14/30: суммарное вознаграждение = 667.72\n",
      "PPO Эпизод 15/30: суммарное вознаграждение = 2275.43\n",
      "PPO Эпизод 16/30: суммарное вознаграждение = 1375.31\n",
      "PPO Эпизод 17/30: суммарное вознаграждение = 73.84\n",
      "PPO Эпизод 18/30: суммарное вознаграждение = 152.00\n",
      "PPO Эпизод 19/30: суммарное вознаграждение = -800.07\n",
      "PPO Эпизод 20/30: суммарное вознаграждение = 4360.95\n",
      "PPO Эпизод 21/30: суммарное вознаграждение = 762.82\n",
      "PPO Эпизод 22/30: суммарное вознаграждение = -1257.17\n",
      "PPO Эпизод 23/30: суммарное вознаграждение = 1795.93\n",
      "PPO Эпизод 24/30: суммарное вознаграждение = 3786.57\n",
      "PPO Эпизод 25/30: суммарное вознаграждение = 629.75\n",
      "PPO Эпизод 26/30: суммарное вознаграждение = 4611.57\n",
      "PPO Эпизод 27/30: суммарное вознаграждение = 2582.07\n",
      "PPO Эпизод 28/30: суммарное вознаграждение = 4388.60\n",
      "PPO Эпизод 29/30: суммарное вознаграждение = 2486.13\n",
      "PPO Эпизод 30/30: суммарное вознаграждение = 3758.31\n"
     ]
    }
   ],
   "source": [
    "# Определяем дискретные наборы параметров для оптимизации\n",
    "gamma_values = [0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "kappa_values = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 1e-1]\n",
    "vol_corr_values = [0.5, 0.75, 1.0, 1.25, 1.5]  # поправка к sigma\n",
    "\n",
    "# Создаем окружение с новым мультидискретным пространством действий\n",
    "env = MarketMakingEnv(\n",
    "    df=df,\n",
    "    gamma_values=gamma_values,\n",
    "    kappa_values=kappa_values,\n",
    "    vol_corr_values=vol_corr_values,\n",
    "    T=60.0,\n",
    ")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "if isinstance(env.action_space, spaces.MultiDiscrete):\n",
    "    action_dim = int(np.prod(env.action_space.nvec))\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "# Инициализируем агентов DQN, A2C и PPO\n",
    "dqn_agent = DQNAgent(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    buffer_capacity=10000,\n",
    "    batch_size=64,\n",
    "    target_update=1000,\n",
    ")\n",
    "a2c_agent = A2CAgent(\n",
    "    state_dim, action_dim, lr=1e-3, gamma=0.99, value_coef=0.5, entropy_coef=0.01\n",
    ")\n",
    "ppo_agent = PPOAgent(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    clip_coef=0.2,\n",
    "    value_coef=0.5,\n",
    "    entropy_coef=0.01,\n",
    "    ppo_epochs=4,\n",
    ")\n",
    "\n",
    "# Параметры обучения\n",
    "num_episodes = 30  # для DQN\n",
    "num_episodes_a2c = 30  # для A2C\n",
    "num_episodes_ppo = 30  # для PPO\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 300\n",
    "\n",
    "# Обучение DQN-агента\n",
    "print(\"Обучение DQN-агента...\")\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        epsilon = epsilon_final + (epsilon_start - epsilon_final) * math.exp(\n",
    "            -env.t / epsilon_decay\n",
    "        )\n",
    "        action = dqn_agent.select_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        dqn_agent.push_transition(state, action, reward, next_state, done)\n",
    "        dqn_agent.update()\n",
    "        state = next_state if next_state is not None else state\n",
    "        total_reward += reward\n",
    "    print(\n",
    "        f\"DQN Эпизод {episode+1}/{num_episodes}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "# Обучение A2C-агента\n",
    "print(\"\\nОбучение A2C-агента...\")\n",
    "rollout_length = 10\n",
    "for episode in range(num_episodes_a2c):\n",
    "    state = env.reset()\n",
    "    trajectories = []\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        for _ in range(rollout_length):\n",
    "            action, log_prob, value, logits = a2c_agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectories.append(\n",
    "                (state, action, log_prob, value, reward, float(done), logits)\n",
    "            )\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        a2c_agent.update(trajectories)\n",
    "        trajectories = []\n",
    "    print(\n",
    "        f\"A2C Эпизод {episode+1}/{num_episodes_a2c}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "# Обучение PPO-агента\n",
    "print(\"\\nОбучение PPO-агента...\")\n",
    "rollout_length_ppo = 20\n",
    "for episode in range(num_episodes_ppo):\n",
    "    state = env.reset()\n",
    "    trajectories = []\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        for _ in range(rollout_length_ppo):\n",
    "            action, log_prob, value = ppo_agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectories.append((state, action, log_prob, value, reward, float(done)))\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        ppo_agent.update(trajectories)\n",
    "        trajectories = []\n",
    "    print(\n",
    "        f\"PPO Эпизод {episode+1}/{num_episodes_ppo}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты тестирования:\n",
      "DQN итоговый PnL: 2.99\n",
      "A2C итоговый PnL: 62.84\n",
      "PPO итоговый PnL: 70.39\n"
     ]
    }
   ],
   "source": [
    "# Функция для тестового прогона агента и сбора истории PnL\n",
    "def run_agent(env, agent, agent_type=\"dqn\"):\n",
    "    state = env.reset()\n",
    "    pnl_history = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        if agent_type == \"dqn\":\n",
    "            action = agent.select_action(state, epsilon=0.0)\n",
    "        elif agent_type == \"a2c\":\n",
    "            action, _, _, _ = agent.select_action(state)\n",
    "        elif agent_type == \"ppo\":\n",
    "            action, _, _ = agent.select_action(state)\n",
    "        else:\n",
    "            action = 0\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        if state is not None:\n",
    "            pnl_history.append(state[2])\n",
    "        state = next_state if next_state is not None else state\n",
    "    return pnl_history\n",
    "\n",
    "\n",
    "# Тестирование на отложенной выборке\n",
    "env_test = MarketMakingEnv(\n",
    "    df=df_test,\n",
    "    gamma_values=gamma_values,\n",
    "    kappa_values=kappa_values,\n",
    "    vol_corr_values=vol_corr_values,\n",
    "    T=60.0,\n",
    ")\n",
    "dqn_pnl = run_agent(env_test, dqn_agent, agent_type=\"dqn\")\n",
    "env_test = MarketMakingEnv(\n",
    "    df=df_test,\n",
    "    gamma_values=gamma_values,\n",
    "    kappa_values=kappa_values,\n",
    "    vol_corr_values=vol_corr_values,\n",
    "    T=60.0,\n",
    ")\n",
    "a2c_pnl = run_agent(env_test, a2c_agent, agent_type=\"a2c\")\n",
    "env_test = MarketMakingEnv(\n",
    "    df=df_test,\n",
    "    gamma_values=gamma_values,\n",
    "    kappa_values=kappa_values,\n",
    "    vol_corr_values=vol_corr_values,\n",
    "    T=60.0,\n",
    ")\n",
    "ppo_pnl = run_agent(env_test, ppo_agent, agent_type=\"ppo\")\n",
    "\n",
    "print(\"\\nРезультаты тестирования:\")\n",
    "print(f\"DQN итоговый PnL: {dqn_pnl[-1]:.2f}\")\n",
    "print(f\"A2C итоговый PnL: {a2c_pnl[-1]:.2f}\")\n",
    "print(f\"PPO итоговый PnL: {ppo_pnl[-1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_drawdown(pnl_history):\n",
    "    pnl_array = np.array(pnl_history)\n",
    "    running_max = np.maximum.accumulate(pnl_array)\n",
    "    drawdowns = pnl_array - running_max\n",
    "    max_drawdown = drawdowns.min()\n",
    "    return max_drawdown\n",
    "\n",
    "\n",
    "def compute_sharpe_ratio(pnl_history):\n",
    "    pnl_array = np.array(pnl_history)\n",
    "    returns = np.diff(pnl_array)\n",
    "    if returns.std() == 0:\n",
    "        return 0.0\n",
    "    sharpe = returns.mean() / returns.std() * np.sqrt(len(returns))\n",
    "    return sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты тестирования:\n",
      "DQN итоговый PnL: 2.99, Max Drawdown: -23.97, Sharpe Ratio: 0.12\n",
      "A2C итоговый PnL: 62.84, Max Drawdown: -23.98, Sharpe Ratio: 1.08\n",
      "PPO итоговый PnL: 70.39, Max Drawdown: -102.44, Sharpe Ratio: 0.45\n"
     ]
    }
   ],
   "source": [
    "dqn_max_dd = compute_max_drawdown(dqn_pnl)\n",
    "dqn_sharpe = compute_sharpe_ratio(dqn_pnl)\n",
    "a2c_max_dd = compute_max_drawdown(a2c_pnl)\n",
    "a2c_sharpe = compute_sharpe_ratio(a2c_pnl)\n",
    "ppo_max_dd = compute_max_drawdown(ppo_pnl)\n",
    "ppo_sharpe = compute_sharpe_ratio(ppo_pnl)\n",
    "\n",
    "print(\"\\nРезультаты тестирования:\")\n",
    "print(\n",
    "    f\"DQN итоговый PnL: {dqn_pnl[-1]:.2f}, Max Drawdown: {dqn_max_dd:.2f}, Sharpe Ratio: {dqn_sharpe:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"A2C итоговый PnL: {a2c_pnl[-1]:.2f}, Max Drawdown: {a2c_max_dd:.2f}, Sharpe Ratio: {a2c_sharpe:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"PPO итоговый PnL: {ppo_pnl[-1]:.2f}, Max Drawdown: {ppo_max_dd:.2f}, Sharpe Ratio: {ppo_sharpe:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:  \n",
    "Подход с 3 настраиваемыми параметрами дал максимальный результат, но результаты все еще не такие оптимистичные, чтобы быть уверенным в положительном мат ожидании прибыли"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
