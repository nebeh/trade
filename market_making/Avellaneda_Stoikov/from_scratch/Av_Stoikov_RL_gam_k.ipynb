{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 time     open     high      low    close     volume  trades\n",
      "0 2025-03-17 15:21:00  1909.42  1910.89  1909.05  1910.41   120.9954  1212.0\n",
      "1 2025-03-17 15:22:00  1910.40  1912.58  1909.60  1911.46   245.0425  1565.0\n",
      "2 2025-03-17 15:23:00  1911.46  1911.77  1910.04  1911.23    99.0537  1030.0\n",
      "3 2025-03-17 15:24:00  1911.22  1913.89  1911.06  1912.63  1619.1776  2335.0\n",
      "4 2025-03-17 15:25:00  1912.63  1914.43  1912.20  1913.68   888.7267  2876.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from binance import Client\n",
    "from gym import spaces\n",
    "\n",
    "# Инициализация клиента Binance (публичные данные без API-ключей)\n",
    "client = Client()\n",
    "\n",
    "\n",
    "# Загрузка исторических данных с Binance\n",
    "def get_klines(\n",
    "    symbol=\"ETHUSDT\",\n",
    "    interval=\"1m\",\n",
    "    start_date=\"6 day ago UTC\",\n",
    "    end_date=\"2 day ago UTC\",\n",
    "):\n",
    "    try:\n",
    "        klines = client.get_historical_klines(\n",
    "            symbol=symbol, interval=interval, start_str=start_date, end_str=end_date\n",
    "        )\n",
    "        # Преобразование результатов в DataFrame\n",
    "        columns = [\n",
    "            \"time\",\n",
    "            \"open\",\n",
    "            \"high\",\n",
    "            \"low\",\n",
    "            \"close\",\n",
    "            \"volume\",\n",
    "            \"close_time\",\n",
    "            \"quote_volume\",\n",
    "            \"trades\",\n",
    "            \"taker_buy_base\",\n",
    "            \"taker_buy_quote\",\n",
    "            \"ignore\",\n",
    "        ]\n",
    "        df = pd.DataFrame(klines, columns=columns, dtype=float)\n",
    "        # Конвертация времени в читаемый формат\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"ms\")\n",
    "        return df[[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при получении данных: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Получаем исторические данные (пример: последние 6 дней с интервалом 1 минута, до 2 дней назад)\n",
    "df = get_klines(symbol=\"ETHUSDT\", interval=\"1m\")\n",
    "print(df.head())  # Вывод первых строк для проверки загрузки данных\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "if df is not None:\n",
    "    N = len(df)\n",
    "    ratio = 0.8\n",
    "    df_test = df[int(N * ratio) :].reset_index(drop=True)\n",
    "    df = df[: int(N * ratio)].reset_index(drop=True)\n",
    "else:\n",
    "    # В случае отсутствия данных, инициализируем пустые DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]\n",
    "    )\n",
    "    df_test = pd.DataFrame(\n",
    "        columns=[\"time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Среда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketMakingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Окружение маркет-мейкинга по Avellaneda-Stoikov (без учета задержки исполнения).\n",
    "    Состояние: [inventory, относительное изменение цены, PnL].\n",
    "    Действие: выбор пары индексов (γ, κ) из заданных списков gamma_values и kappa_values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, gamma_values, kappa_values, T=60.0):\n",
    "        super(MarketMakingEnv, self).__init__()\n",
    "        self.prices = df.close.values\n",
    "        self.prices_high = df.high.values\n",
    "        self.prices_low = df.low.values\n",
    "        self.N = len(df)\n",
    "        self.gamma_values = gamma_values  # дискретный набор допустимых γ\n",
    "        self.kappa_values = kappa_values  # дискретный набор допустимых κ\n",
    "        self.T = T  # торговый горизонт (в секундах) для расчёта оптимальных спредов\n",
    "        # Пространство действий: MultiDiscrete для пары индексов [index_gamma, index_kappa]\n",
    "        self.action_space = spaces.MultiDiscrete([len(gamma_values), len(kappa_values)])\n",
    "        # Пространство состояний: содержит инвентарь, относительное изменение цены и накопленный PnL\n",
    "        high = np.array([100.0, 1.0, 1e9], dtype=np.float32)\n",
    "        low = np.array([-100.0, -1.0, -1e9], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "        # Прочие параметры окружения\n",
    "        self.initial_cash = 10000.0\n",
    "        self.fee_rate = 0.0  # комиссия (можно изменить при необходимости)\n",
    "\n",
    "    def reset(self):\n",
    "        # Сброс состояния среды в начало эпизода\n",
    "        self.t = 0\n",
    "        self.inventory = 0.0\n",
    "        self.cash = self.initial_cash\n",
    "        # Начальное наблюдение: [инвентарь, 0 изменение цены, 0 PnL]\n",
    "        return np.array([self.inventory, 0.0, 0.0], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Декодирование мультидискретного действия в значения gamma и kappa\n",
    "        if isinstance(action, (np.ndarray, list, tuple)):\n",
    "            gamma_idx = int(action[0])\n",
    "            kappa_idx = int(action[1])\n",
    "        else:\n",
    "            # Если действие передано как один индекс (например, от DQN), раскодируем его\n",
    "            action = int(action)\n",
    "            gamma_idx = action // len(self.kappa_values)\n",
    "            kappa_idx = action % len(self.kappa_values)\n",
    "        gamma = self.gamma_values[gamma_idx]\n",
    "        kappa = self.kappa_values[kappa_idx]\n",
    "        mid_price = self.prices[self.t]\n",
    "        # Оценка волатильности (стандартное отклонение относительных изменений цены) на окне последних T секунд\n",
    "        sigma = 0.0\n",
    "        if self.t > 1:\n",
    "            start = max(0, self.t - int(self.T))\n",
    "            window_prices = self.prices[start : self.t + 1]\n",
    "            if len(window_prices) > 1:\n",
    "                returns = np.diff(window_prices) / window_prices[:-1]\n",
    "                sigma = np.std(returns)\n",
    "        # Расчёт цены резервирования и оптимального спреда по Avellaneda-Stoikov\n",
    "        # Цена резервирования учитывает риск (gamma) и текущий инвентарь\n",
    "        reservation_price = mid_price - self.inventory * gamma * (sigma**2) * self.T\n",
    "        # Оптимальная половина спреда delta/2 с учётом параметров gamma и kappa\n",
    "        # Формула: delta = γ * σ^2 * T + (1/γ) * ln(1 + γ/κ)\n",
    "        delta = gamma * (sigma**2) * self.T + (1.0 / gamma) * math.log(\n",
    "            1 + gamma / kappa\n",
    "        )\n",
    "        bid_price = reservation_price - delta / 2  # цена бид\n",
    "        ask_price = reservation_price + delta / 2  # цена аск\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "        # Эмуляция исполнения ордеров за следующий шаг\n",
    "        if self.t < self.N - 1:\n",
    "            next_price = self.prices[self.t + 1]\n",
    "            # Если максимальная цена следующего интервала >= нашей ask_price -> ордер на продажу исполнился\n",
    "            if self.prices_high[self.t + 1] >= ask_price:\n",
    "                self.inventory -= 1.0\n",
    "                self.cash += ask_price * (1.0 - self.fee_rate)\n",
    "            # Если минимальная цена следующего интервала <= нашей bid_price -> ордер на покупку исполнился\n",
    "            if self.prices_low[self.t + 1] <= bid_price:\n",
    "                self.inventory += 1.0\n",
    "                self.cash -= bid_price * (1.0 + self.fee_rate)\n",
    "            # Рассчитываем новую оценку портфеля и изменение PnL\n",
    "            current_value = self.cash + self.inventory * next_price\n",
    "            prev_value = self.cash + self.inventory * mid_price\n",
    "            pnl_change = current_value - prev_value\n",
    "            # Небольшой штраф за размер позиции (чтобы ограничивать риск чрезмерного инвентаря)\n",
    "            reward = pnl_change - 0.001 * abs(self.inventory)\n",
    "            # Переходим к следующему шагу времени\n",
    "            self.t += 1\n",
    "        else:\n",
    "            # Если достигли конца данных - завершаем эпизод\n",
    "            done = True\n",
    "            current_value = self.cash + self.inventory * mid_price\n",
    "            # Финальный PnL за эпизод является вознаграждением на последнем шаге\n",
    "            reward = current_value - self.initial_cash\n",
    "        # Формирование нового состояния\n",
    "        if not done:\n",
    "            new_mid = self.prices[self.t]\n",
    "            price_change = (\n",
    "                new_mid - mid_price\n",
    "            ) / mid_price  # относительное изменение цены\n",
    "            pnl = self.cash + self.inventory * new_mid - self.initial_cash\n",
    "            obs = np.array([self.inventory, price_change, pnl], dtype=np.float32)\n",
    "        else:\n",
    "            obs = None\n",
    "        return obs, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Агенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== DQN агент ====\n",
    "\n",
    "# Q-сеть (критик) для DQN: простая полносвязная сеть\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "# Переход для буфера воспроизведения\n",
    "Transition = namedtuple(\n",
    "    \"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Буфер воспроизведения для DQN\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        buffer_capacity=10000,\n",
    "        batch_size=64,\n",
    "        target_update=1000,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.steps_done = 0\n",
    "        self.action_dim = (\n",
    "            action_dim  # общее количество возможных действий (комбинаций γ и κ)\n",
    "        )\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        # ε-жадная стратегия выбора действия\n",
    "        if random.random() < epsilon:\n",
    "            # случайное действие (исследование)\n",
    "            return random.randrange(self.action_dim)\n",
    "        else:\n",
    "            # действие по текущей Q-оценке (эксплуатация)\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def push_transition(self, state, action, reward, next_state, done):\n",
    "        # Сохраняем переход в буфер\n",
    "        self.buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def update(self):\n",
    "        # Обновление сети DQN из буфера воспроизведения\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return  # недостаточно данных для обучения\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # Подготовка тензоров для обучения\n",
    "        state_batch = torch.FloatTensor(batch.state).to(self.device)\n",
    "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).unsqueeze(1).to(self.device)\n",
    "        # Формируем маску для неконечных состояний и тензор следующего состояния\n",
    "        non_final_mask = torch.tensor(\n",
    "            [s is not None for s in batch.next_state],\n",
    "            dtype=torch.bool,\n",
    "            device=self.device,\n",
    "        )\n",
    "        non_final_next_states = torch.FloatTensor(\n",
    "            [s for s in batch.next_state if s is not None]\n",
    "        ).to(self.device)\n",
    "        done_batch = torch.FloatTensor(batch.done).unsqueeze(1).to(self.device)\n",
    "        # Вычисляем текущие Q(s, a) и целевые Q-значения\n",
    "        q_values = self.q_network(state_batch).gather(1, action_batch)  # Q(s_t, a_t)\n",
    "        next_q_values = torch.zeros(self.batch_size, 1).to(self.device)\n",
    "        if non_final_next_states.size(0) > 0:\n",
    "            # Максимальное Q для следующего состояния (по целевой сети)\n",
    "            next_q_values[non_final_mask] = self.target_network(\n",
    "                non_final_next_states\n",
    "            ).max(1, keepdim=True)[0]\n",
    "        expected_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
    "        # Потеря: MSE между текущим Q и целевым Q\n",
    "        loss = nn.MSELoss()(q_values, expected_q_values.detach())\n",
    "        # Обновление Q-сети\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # Периодическое обновление целевой сети\n",
    "        self.steps_done += 1\n",
    "        if self.steps_done % self.target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "\n",
    "# ==== A2C агент ====\n",
    "\n",
    "\n",
    "# Актор-критик сеть (общие слои, отдельные выходы actor и critic)\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.actor = nn.Linear(64, action_dim)  # выход для политики (логиты действий)\n",
    "        self.critic = nn.Linear(64, 1)  # выход для ценности (V(s))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ActorCritic(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, value = self.model(state_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        # Сэмплируем действие из распределения\n",
    "        action_tensor = torch.multinomial(probs, num_samples=1)\n",
    "        log_prob = torch.log(probs.gather(1, action_tensor) + 1e-10)\n",
    "        # Возвращаем выбранное действие и связанные значения для обучения\n",
    "        return (\n",
    "            action_tensor.item(),\n",
    "            log_prob.squeeze(),\n",
    "            value.squeeze(),\n",
    "            logits.squeeze(),\n",
    "        )\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        # trajectories: список кортежей (state, action, log_prob, value, reward, done, logits)\n",
    "        log_probs = torch.stack([t[2] for t in trajectories]).to(self.device)\n",
    "        values = torch.stack([t[3] for t in trajectories]).to(self.device)\n",
    "        rewards = [t[4] for t in trajectories]\n",
    "        dones = [t[5] for t in trajectories]\n",
    "        logits_list = torch.stack([t[6] for t in trajectories]).to(self.device)\n",
    "        # Расчёт накопленных вознаграждений (Returns) методом обратного прохода по траектории\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = returns - values\n",
    "        # Потери актора и критика\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        # Энтропия политики (для поощрения исследования)\n",
    "        probs = torch.softmax(logits_list, dim=1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "        # Суммарный loss\n",
    "        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "        # Обновление параметров сети\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "# ==== PPO агент ====\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        clip_coef=0.2,\n",
    "        value_coef=0.5,\n",
    "        entropy_coef=0.01,\n",
    "        ppo_epochs=4,\n",
    "    ):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ActorCritic(state_dim, action_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_coef = clip_coef  # коэффициент клиппинга PPO (ε)\n",
    "        self.value_coef = value_coef  # коэффициент для критика\n",
    "        self.entropy_coef = entropy_coef  # коэффициент для энтропии\n",
    "        self.ppo_epochs = ppo_epochs  # число эпох обновления на одном наборе траекторий\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, value = self.model(state_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        # Семплируем действие из текущей политики\n",
    "        action_tensor = torch.multinomial(probs, num_samples=1)\n",
    "        log_prob = torch.log(probs.gather(1, action_tensor) + 1e-10)\n",
    "        # Возвращаем выбранное действие (скаляр) и связанные величины для хранения траектории\n",
    "        return action_tensor.item(), log_prob.squeeze(), value.squeeze()\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        # trajectories: список кортежей (state, action, log_prob, value, reward, done)\n",
    "        # Формируем тензоры из собранной траектории\n",
    "        states = torch.FloatTensor([t[0] for t in trajectories]).to(self.device)\n",
    "        actions = (\n",
    "            torch.LongTensor([t[1] for t in trajectories]).unsqueeze(1).to(self.device)\n",
    "        )\n",
    "        old_log_probs = torch.stack([t[2] for t in trajectories]).to(self.device)\n",
    "        old_values = torch.stack([t[3] for t in trajectories]).to(self.device)\n",
    "        rewards = [t[4] for t in trajectories]\n",
    "        dones = [t[5] for t in trajectories]\n",
    "        # Вычисляем массив Returns по траектории (обратным проходом)\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            R = reward + self.gamma * R * (1 - done)\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        # Вычисляем преимущества (Advantage = Return - baseline)\n",
    "        # Старые log_prob и value считаем константами на время обновления (без градиентов)\n",
    "        old_log_probs = old_log_probs.detach()\n",
    "        old_values = old_values.detach()\n",
    "        advantages = returns - old_values\n",
    "        # Необязательная нормализация advantages:\n",
    "        # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        # Многократное обновление политики и ценности на одних и тех же данных траектории (PPO Epochs)\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            logits, values = self.model(states)\n",
    "            values = values.squeeze(1)  # приведение размеров ценности к [batch]\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            # Новые лог-вероятности выбранных действий по обновленной политике\n",
    "            new_log_probs = torch.log(probs.gather(1, actions) + 1e-10).squeeze(1)\n",
    "            # Вычисляем отношение вероятностей π_new/π_old для каждого шага\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            # Вычисляем функции успеха (surrogate) с клиппингом\n",
    "            adv_detached = advantages.detach()\n",
    "            surr1 = ratio * adv_detached\n",
    "            surr2 = (\n",
    "                torch.clamp(ratio, 1.0 - self.clip_coef, 1.0 + self.clip_coef)\n",
    "                * adv_detached\n",
    "            )\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            # Потеря критика: MSE между прогнозом ценности и подсчитанным возвратом\n",
    "            critic_loss = (returns - values).pow(2).mean()\n",
    "            # Энтропия политики (чем выше, тем лучше исследование)\n",
    "            entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=1).mean()\n",
    "            # Полный loss с учетом коэффициентов\n",
    "            loss = (\n",
    "                actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "            )\n",
    "            # Обновляем параметры модели\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение DQN-агента...\n",
      "Эпизод 1/20: суммарное вознаграждение = 1137.85\n",
      "Эпизод 2/20: суммарное вознаграждение = 1128.52\n",
      "Эпизод 3/20: суммарное вознаграждение = 457.06\n",
      "Эпизод 4/20: суммарное вознаграждение = -3.16\n",
      "Эпизод 5/20: суммарное вознаграждение = 1652.17\n",
      "Эпизод 6/20: суммарное вознаграждение = -614.21\n",
      "Эпизод 7/20: суммарное вознаграждение = 656.15\n",
      "Эпизод 8/20: суммарное вознаграждение = -151.40\n",
      "Эпизод 9/20: суммарное вознаграждение = 133.54\n",
      "Эпизод 10/20: суммарное вознаграждение = 967.93\n",
      "Эпизод 11/20: суммарное вознаграждение = 1510.01\n",
      "Эпизод 12/20: суммарное вознаграждение = 1347.65\n",
      "Эпизод 13/20: суммарное вознаграждение = 1610.03\n",
      "Эпизод 14/20: суммарное вознаграждение = 1336.57\n",
      "Эпизод 15/20: суммарное вознаграждение = -593.58\n",
      "Эпизод 16/20: суммарное вознаграждение = 1744.52\n",
      "Эпизод 17/20: суммарное вознаграждение = 1574.68\n",
      "Эпизод 18/20: суммарное вознаграждение = 1338.45\n",
      "Эпизод 19/20: суммарное вознаграждение = 972.15\n",
      "Эпизод 20/20: суммарное вознаграждение = 1656.64\n",
      "\n",
      "Обучение A2C-агента...\n",
      "Эпизод 1/20: суммарное вознаграждение = 2231.14\n",
      "Эпизод 2/20: суммарное вознаграждение = 1353.92\n",
      "Эпизод 3/20: суммарное вознаграждение = 1741.01\n",
      "Эпизод 4/20: суммарное вознаграждение = 1262.57\n",
      "Эпизод 5/20: суммарное вознаграждение = 1910.75\n",
      "Эпизод 6/20: суммарное вознаграждение = 1515.20\n",
      "Эпизод 7/20: суммарное вознаграждение = 737.98\n",
      "Эпизод 8/20: суммарное вознаграждение = 2170.71\n",
      "Эпизод 9/20: суммарное вознаграждение = 411.67\n",
      "Эпизод 10/20: суммарное вознаграждение = 2073.43\n",
      "Эпизод 11/20: суммарное вознаграждение = 241.52\n",
      "Эпизод 12/20: суммарное вознаграждение = 857.81\n",
      "Эпизод 13/20: суммарное вознаграждение = 447.33\n",
      "Эпизод 14/20: суммарное вознаграждение = 1942.42\n",
      "Эпизод 15/20: суммарное вознаграждение = 125.97\n",
      "Эпизод 16/20: суммарное вознаграждение = 503.87\n",
      "Эпизод 17/20: суммарное вознаграждение = 2517.57\n",
      "Эпизод 18/20: суммарное вознаграждение = -683.38\n",
      "Эпизод 19/20: суммарное вознаграждение = 559.99\n",
      "Эпизод 20/20: суммарное вознаграждение = -158.12\n",
      "\n",
      "Обучение PPO-агента...\n",
      "Эпизод 1/20: суммарное вознаграждение = 465.23\n",
      "Эпизод 2/20: суммарное вознаграждение = -572.25\n",
      "Эпизод 3/20: суммарное вознаграждение = 111.49\n",
      "Эпизод 4/20: суммарное вознаграждение = 613.79\n",
      "Эпизод 5/20: суммарное вознаграждение = -140.56\n",
      "Эпизод 6/20: суммарное вознаграждение = 331.72\n",
      "Эпизод 7/20: суммарное вознаграждение = 104.04\n",
      "Эпизод 8/20: суммарное вознаграждение = 386.30\n",
      "Эпизод 9/20: суммарное вознаграждение = 592.11\n",
      "Эпизод 10/20: суммарное вознаграждение = 1337.36\n",
      "Эпизод 11/20: суммарное вознаграждение = 442.78\n",
      "Эпизод 12/20: суммарное вознаграждение = 1537.16\n",
      "Эпизод 13/20: суммарное вознаграждение = 850.66\n",
      "Эпизод 14/20: суммарное вознаграждение = -280.54\n",
      "Эпизод 15/20: суммарное вознаграждение = 1199.22\n",
      "Эпизод 16/20: суммарное вознаграждение = 168.86\n",
      "Эпизод 17/20: суммарное вознаграждение = -30.20\n",
      "Эпизод 18/20: суммарное вознаграждение = 533.75\n",
      "Эпизод 19/20: суммарное вознаграждение = 375.85\n",
      "Эпизод 20/20: суммарное вознаграждение = -266.91\n"
     ]
    }
   ],
   "source": [
    "# Задаём дискретные значения параметров gamma и kappa для оптимизации\n",
    "gamma_values = [0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "kappa_values = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
    "\n",
    "# Создаём окружение с мультидискретными действиями\n",
    "env = MarketMakingEnv(\n",
    "    df=df, gamma_values=gamma_values, kappa_values=kappa_values, T=60.0\n",
    ")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "# Определяем размер действия для сетей агентов (количество комбинаций gamma-kappa)\n",
    "if isinstance(env.action_space, spaces.MultiDiscrete):\n",
    "    action_dim = int(np.prod(env.action_space.nvec))\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "# Инициализируем агентов DQN, A2C и PPO\n",
    "dqn_agent = DQNAgent(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    buffer_capacity=10000,\n",
    "    batch_size=64,\n",
    "    target_update=1000,\n",
    ")\n",
    "a2c_agent = A2CAgent(\n",
    "    state_dim, action_dim, lr=1e-3, gamma=0.99, value_coef=0.5, entropy_coef=0.01\n",
    ")\n",
    "ppo_agent = PPOAgent(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    clip_coef=0.2,\n",
    "    value_coef=0.5,\n",
    "    entropy_coef=0.01,\n",
    "    ppo_epochs=4,\n",
    ")\n",
    "\n",
    "# Параметры обучения\n",
    "num_episodes = 20  # эпизодов для DQN\n",
    "num_episodes_a2c = 20  # эпизодов для A2C\n",
    "num_episodes_ppo = 20  # эпизодов для PPO\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 300\n",
    "\n",
    "# Обучение DQN-агента\n",
    "print(\"Обучение DQN-агента...\")\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Экспоненциальное уменьшение ε в ходе эпизода\n",
    "        epsilon = epsilon_final + (epsilon_start - epsilon_final) * math.exp(\n",
    "            -env.t / epsilon_decay\n",
    "        )\n",
    "        action = dqn_agent.select_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        dqn_agent.push_transition(state, action, reward, next_state, done)\n",
    "        dqn_agent.update()\n",
    "        state = next_state if next_state is not None else state\n",
    "        total_reward += reward\n",
    "    print(\n",
    "        f\"Эпизод {episode+1}/{num_episodes}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "# Обучение A2C-агента\n",
    "print(\"\\nОбучение A2C-агента...\")\n",
    "rollout_length = 10  # длина сегмента траектории перед обновлением\n",
    "for episode in range(num_episodes_a2c):\n",
    "    state = env.reset()\n",
    "    trajectories = []\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Сбор траектории длины rollout_length\n",
    "        for _ in range(rollout_length):\n",
    "            action, log_prob, value, logits = a2c_agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # Сохраняем данные шага для последующего обучения\n",
    "            trajectories.append(\n",
    "                (state, action, log_prob, value, reward, float(done), logits)\n",
    "            )\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        # Обновляем агент A2C по собранной траектории\n",
    "        a2c_agent.update(trajectories)\n",
    "        trajectories = []  # очищаем для сбора следующего сегмента\n",
    "    print(\n",
    "        f\"Эпизод {episode+1}/{num_episodes_a2c}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n",
    "\n",
    "# Обучение PPO-агента\n",
    "print(\"\\nОбучение PPO-агента...\")\n",
    "rollout_length_ppo = 20  # длина сегмента траектории для PPO перед обновлением\n",
    "for episode in range(num_episodes_ppo):\n",
    "    state = env.reset()\n",
    "    trajectories = []\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Сбор траектории длиной rollout_length_ppo\n",
    "        for _ in range(rollout_length_ppo):\n",
    "            action, log_prob, value = ppo_agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            trajectories.append((state, action, log_prob, value, reward, float(done)))\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        # Обновляем агент PPO по собранному отрезку траектории (с многократным прогонам PPO)\n",
    "        ppo_agent.update(trajectories)\n",
    "        trajectories = []  # очищаем перед сбором следующего сегмента\n",
    "    print(\n",
    "        f\"Эпизод {episode+1}/{num_episodes_ppo}: суммарное вознаграждение = {total_reward:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для прогона тестового эпизода и сбора истории PnL\n",
    "def run_agent(env, agent, agent_type=\"dqn\"):\n",
    "    state = env.reset()\n",
    "    pnl_history = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        if agent_type == \"dqn\":\n",
    "            # Для DQN берём действие жадно (epsilon=0 для выбора лучшего действия)\n",
    "            action = agent.select_action(state, epsilon=0.0)\n",
    "        elif agent_type == \"a2c\":\n",
    "            # Для A2C и PPO можно брать среднее по политике; здесь для простоты тоже сэмплируем\n",
    "            action, _, _, _ = agent.select_action(state)\n",
    "        elif agent_type == \"ppo\":\n",
    "            action, _, _ = agent.select_action(state)\n",
    "        else:\n",
    "            # Базовая стратегия (например, всегда берём первые значения gamma и kappa)\n",
    "            action = 0\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        # Третий элемент состояния (индекс 2) хранит текущий PnL\n",
    "        if state is not None:\n",
    "            pnl_history.append(state[2])\n",
    "        state = next_state if next_state is not None else state\n",
    "    return pnl_history\n",
    "\n",
    "\n",
    "# Тестирование агентов на отложенной выборке\n",
    "env_test = MarketMakingEnv(\n",
    "    df=df_test, gamma_values=gamma_values, kappa_values=kappa_values, T=60.0\n",
    ")\n",
    "dqn_pnl = run_agent(env_test, dqn_agent, agent_type=\"dqn\")\n",
    "env_test = MarketMakingEnv(\n",
    "    df=df_test, gamma_values=gamma_values, kappa_values=kappa_values, T=60.0\n",
    ")\n",
    "a2c_pnl = run_agent(env_test, a2c_agent, agent_type=\"a2c\")\n",
    "env_test = MarketMakingEnv(\n",
    "    df=df_test, gamma_values=gamma_values, kappa_values=kappa_values, T=60.0\n",
    ")\n",
    "ppo_pnl = run_agent(env_test, ppo_agent, agent_type=\"ppo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Результаты тестирования:\n",
      "DQN итоговый PnL: 1.63, Max Drawdown: -16.43, Sharpe Ratio: 0.07\n",
      "A2C итоговый PnL: 25.61, Max Drawdown: -21.30, Sharpe Ratio: 0.87\n",
      "PPO итоговый PnL: -41.29, Max Drawdown: -81.20, Sharpe Ratio: -0.56\n"
     ]
    }
   ],
   "source": [
    "def compute_max_drawdown(pnl_history):\n",
    "    pnl_array = np.array(pnl_history)\n",
    "    running_max = np.maximum.accumulate(pnl_array)\n",
    "    drawdowns = pnl_array - running_max\n",
    "    max_drawdown = drawdowns.min()\n",
    "    return max_drawdown\n",
    "\n",
    "\n",
    "def compute_sharpe_ratio(pnl_history):\n",
    "    pnl_array = np.array(pnl_history)\n",
    "    returns = np.diff(pnl_array)\n",
    "    if returns.std() == 0:\n",
    "        return 0.0\n",
    "    sharpe = returns.mean() / returns.std() * np.sqrt(len(returns))\n",
    "    return sharpe\n",
    "\n",
    "\n",
    "dqn_max_dd = compute_max_drawdown(dqn_pnl)\n",
    "dqn_sharpe = compute_sharpe_ratio(dqn_pnl)\n",
    "a2c_max_dd = compute_max_drawdown(a2c_pnl)\n",
    "a2c_sharpe = compute_sharpe_ratio(a2c_pnl)\n",
    "ppo_max_dd = compute_max_drawdown(ppo_pnl)\n",
    "ppo_sharpe = compute_sharpe_ratio(ppo_pnl)\n",
    "\n",
    "print(\"\\nРезультаты тестирования:\")\n",
    "print(\n",
    "    f\"DQN итоговый PnL: {dqn_pnl[-1]:.2f}, Max Drawdown: {dqn_max_dd:.2f}, Sharpe Ratio: {dqn_sharpe:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"A2C итоговый PnL: {a2c_pnl[-1]:.2f}, Max Drawdown: {a2c_max_dd:.2f}, Sharpe Ratio: {a2c_sharpe:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"PPO итоговый PnL: {ppo_pnl[-1]:.2f}, Max Drawdown: {ppo_max_dd:.2f}, Sharpe Ratio: {ppo_sharpe:.2f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
